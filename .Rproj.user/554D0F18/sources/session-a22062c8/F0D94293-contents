setwd('/Users/zhangxuan/Desktop/Zhang Lab/language competition/diachronic_sense_modeling/data')
setwd('D:/Fudan/Zhang Lab/language competition/diachronic_sense_modeling/data')
load('data_prop_fittingg_10.Rdata')

it1ws5 <- read.csv('C:/Users/84254/Desktop/投稿/sense_ratio_it1ws5.csv')
it1ws5_data <- make_polysemous_data(it1ws5)
save(it1ws5_data, file = 'it1ws5_data.Rdata')

it1ws5_data_new <- make_polysemous_data(it1ws5)
save(it1ws5_data_new, file = 'it1ws5_data_new.Rdata')

## Required packages
library(dplyr)
library(purrr)
library(tidyr)
library(stringr)
library(tibble)
make_polysemous_data <- function(df) {
  # deps
  if (!requireNamespace("tidyverse", quietly = TRUE)) {
    stop("Please install.packages('tidyverse') first.")
  }
  library(tidyverse)

  # helper: parse x/y cell into numeric vector (supports list, numeric, "[...]" char)
  parse_vec <- function(v, as_integer = FALSE) {
    if (is.list(v)) v <- v[[1]]
    if (is.numeric(v)) out <- v else {
      # NOTE: generic decimal-only regex kept for x and other fields
      nums <- stringr::str_extract_all(as.character(v), "-?\\d+\\.?\\d*")[[1]]
      out <- as.numeric(nums)
    }
    if (as_integer) out <- as.integer(out)
    out
  }

  # ---- NEW: dedicated parser for y that preserves scientific notation ----
  parse_y_vec <- function(v) {                                                # <-- CHANGED (new helper)
    if (is.list(v)) v <- v[[1]]                                              # <-- CHANGED
    ch <- as.character(v)                                                    # <-- CHANGED (force character first)
    # exponent-aware pattern: optional sign, digits, optional dot, digits, optional exponent
    nums <- stringr::str_extract_all(ch, "-?\\d*\\.?\\d+(?:[eE][+-]?\\d+)?")[[1]]  # <-- CHANGED
    as.numeric(nums)                                                         # <-- CHANGED
  }
  # -----------------------------------------------------------------------

  # 1) build polysemous_data with wide df per word
  res <- df %>%
    dplyr::group_split(.data$word, .keep = TRUE) %>%
    purrr::map(function(dfw) {
      w <- dfw$word[1]

      senses_list <- split(dfw, dfw$sense_id) %>%
        purrr::imap(function(sdf, sid) list(
          definition = sdf$definition[[1]],
          x = parse_vec(sdf$x, as_integer = TRUE),
          y = parse_y_vec(sdf$y)                                            # <-- CHANGED (use y-specific parser)
        ))

      all_years <- sort(unique(unlist(purrr::map(senses_list, "x"))))
      wide <- tibble::tibble(year = all_years)
      for (sid in names(senses_list)) {
        xi <- senses_list[[sid]]$x
        yi <- senses_list[[sid]]$y
        v <- rep(NA_real_, length(all_years))
        v[match(xi, all_years)] <- yi
        wide[[sid]] <- v
      }

      list(word = w, senses = senses_list, n_senses = length(senses_list), df = wide)
    }) %>%
    purrr::set_names(purrr::map_chr(., "word"))

  # 2) replace each df with 3 columns: year / sense1 (max col of first row) / sense2 = 1 - sense1
  res <- purrr::map(res, function(item) {
    dfw <- item$df
    stopifnot("year" %in% names(dfw))
    sense_cols <- setdiff(names(dfw), "year")

    # choose column with max value in the first row (ignore NA; if all NA, fallback to first sense col)
    row1 <- suppressWarnings(as.numeric(dfw[1, sense_cols]))
    if (all(is.na(row1))) {
      max_col <- sense_cols[1]
    } else {
      row1[is.na(row1)] <- -Inf
      max_col <- sense_cols[which.max(row1)]
    }

    item$df <- tibble::tibble(
      year   = dfw$year,
      sense1 = as.numeric(dfw[[max_col]]),
      sense2 = 1 - as.numeric(dfw[[max_col]])
    )
    item
  })
  res
}

# 下面的不对，识别不了科学计数法
make_polysemous_data <- function(df) {
  # deps
  if (!requireNamespace("tidyverse", quietly = TRUE)) {
    stop("Please install.packages('tidyverse') first.")
  }
  library(tidyverse)

  # helper: parse x/y cell into numeric vector (supports list, numeric, "[...]" char)
  parse_vec <- function(v, as_integer = FALSE) {
    if (is.list(v)) v <- v[[1]]
    if (is.numeric(v)) out <- v else {
      nums <- stringr::str_extract_all(as.character(v), "-?\\d+\\.?\\d*")[[1]]
      out <- as.numeric(nums)
    }
    if (as_integer) out <- as.integer(out)
    out
  }

  # 1) build polysemous_data with wide df per word
  res <- df %>%
    dplyr::group_split(.data$word, .keep = TRUE) %>%
    purrr::map(function(dfw) {
      w <- dfw$word[1]

      senses_list <- split(dfw, dfw$sense_id) %>%
        purrr::imap(function(sdf, sid) list(
          definition = sdf$definition[[1]],
          x = parse_vec(sdf$x, as_integer = TRUE),
          y = parse_vec(sdf$y, as_integer = FALSE)
        ))

      all_years <- sort(unique(unlist(purrr::map(senses_list, "x"))))
      wide <- tibble::tibble(year = all_years)
      for (sid in names(senses_list)) {
        xi <- senses_list[[sid]]$x
        yi <- senses_list[[sid]]$y
        v <- rep(NA_real_, length(all_years))
        v[match(xi, all_years)] <- yi
        wide[[sid]] <- v
      }

      list(word = w, senses = senses_list, n_senses = length(senses_list), df = wide)
    }) %>%
    purrr::set_names(purrr::map_chr(., "word"))

  # 2) replace each df with 3 columns: year / sense1 (max col of first row) / sense2 = 1 - sense1
  res <- purrr::map(res, function(item) {
    dfw <- item$df
    stopifnot("year" %in% names(dfw))
    sense_cols <- setdiff(names(dfw), "year")

    # choose column with max value in the first row (ignore NA; if all NA, fallback to first sense col)
    row1 <- suppressWarnings(as.numeric(dfw[1, sense_cols]))
    if (all(is.na(row1))) {
      max_col <- sense_cols[1]
    } else {
      row1[is.na(row1)] <- -Inf
      max_col <- sense_cols[which.max(row1)]
    }

    item$df <- tibble::tibble(
      year   = dfw$year,
      sense1 = as.numeric(dfw[[max_col]]),
      sense2 = 1 - as.numeric(dfw[[max_col]])
    )
    item
  })
  res
}

############# polysemous_data 3192 #################

# 去除3220个词中只有1年数据的26个词
words_with_x_length_one <- c()

# 遍历prop_fitting_10列表
for (i in seq_along(prop_fitting_10)) {
  # 获取当前元素的word字段作为词的名称
  current_word <- prop_fitting_10[[i]]$word

  # 遍历当前词的所有senses
  senses <- prop_fitting_10[[i]]$senses
  for (sense_name in names(senses)) {
    # 检查x字段的长度是否为1
    if (length(senses[[sense_name]]$x) == 1) {
      # 如果是，将当前词的名称添加到结果向量中
      words_with_x_length_one <- c(words_with_x_length_one, current_word)
      break  # 已找到至少一个符合条件的sense，跳出内层循环
    }
  }
}

# 去除3220个词中只有1个义项的2个词
words_with_one_sense <- c()
for (i in seq_along(prop_fitting_10)) {
  current_word <- prop_fitting_10[[i]]$word
  senses <- prop_fitting_10[[i]]$senses
  if (length(senses) == 1) {
    words_with_one_sense <- c(words_with_one_sense, current_word)
  }
}

words_to_remove <- unique(c(words_with_x_length_one, words_with_one_sense))
prop_fitting_10 <- Filter(function(item) {
   !(item$word %in% words_to_remove)
}, prop_fitting_10)

# 共计3,192个词
polysemous_data <- lapply(prop_fitting_10, function(item) {
  item$senses <- lapply(item$senses, function(sense) {
    if ("y_fitting" %in% names(sense)) {
      sense$y_fitting <- NULL
    }
    return(sense)
  })
  return(item)
})


# Original data of lexical semantics generated by our tracking framework
save(polysemous_data, file = 'polysemous_data.Rdata')




###################### interpolation #######################
library(dplyr)
polysemous_data <- lapply(polysemous_data, function(result) {
  # Extract the first definition of each sense and calculate the total number of senses
  def_senses <- sapply(result$senses, '[', 1)
  result$n_senses <- length(def_senses)

  # Extract the years and associated sense data
  years <- unlist(sapply(result$senses[1], '[', 2))
  x_senses <- sapply(result[[2]], '[', 3)

  # Identify the initial proportions of each sense
  first_elements <- sapply(x_senses, `[`, 1)
  sorted_list_names <- names(first_elements)[order(first_elements, decreasing = TRUE)]
  x_dominant_sense <- x_senses[[sorted_list_names[1]]]

  # Perform spline interpolation on the data
  interpolation <- spline(years, x_dominant_sense, n = length(seq(years[1], years[length(years)], by = 2)))

  # Extract the interpolated years and sense values
  years_interpolation <- interpolation$x
  sense1 <- interpolation$y

  # Create a dataframe for the interpolated values
  df_interpolation <- data.frame(year = years_interpolation, sense1 = sense1, sense2 = 1 - sense1)

  # Normalize sense1 and sense2 values to the range [0, 1]
  df_interpolation[, 2] <- pmax(0, pmin(1, df_interpolation[, 2]))
  df_interpolation[, 3] <- 1 - df_interpolation[, 2]

  # Assign the interpolated dataframe to the result
  result$df <- df_interpolation
  return(result)
})



for (i in 1:length(polysemous_data)) {
  result <- polysemous_data[[i]]   # !!!! input

  def_senses <- sapply(result$senses, '[', 1)
  polysemous_data[[i]]$n_senses <- length(def_senses)

  years <- as.vector(unlist(sapply(result$senses[1], '[', 2)))
  x_senses <- sapply(result[[2]], '[', 3)


  first_elements <- sapply(x_senses, `[`, 1)  # find the initial proportion of each sense
  sorted_list_names <- names(first_elements)[order(first_elements, decreasing = TRUE)]
  x_dominant_sense <- x_senses[[sorted_list_names[1]]]

  interpolation <- spline(years, x_dominant_sense, n=length(seq(years[1], years[length(years)], by = 2)))

  years_interpolation <- interpolation$x
  sense1 <- interpolation$y

  df_interpolation <- data.frame(year = years_interpolation, sense1 = sense1, sense2 = 1-sense1)
  df_interpolation[, 2] <- pmax(0, pmin(1, df_interpolation[, 2]))
  df_interpolation[, 3] <- 1 - df_interpolation[, 2]

  polysemous_data[[i]]$df <- df_interpolation

}
save(polysemous_data, file = 'polysemous_data.Rdata')

it1ws5_data_v2

###################### sde parametric estimation #######################
library(fmcmc)            # MCMC()
library(MASS)             # ginv()
library(coda)             # geweke.diag()

# Define a function to compute the logarithm of the conditional probability density function.
log.cond.pdf <- function(xt1, xt2, x1, x2, dt, drift.vec, diff.vec){
  Q <- diag(c(drift.vec[1]*x2*(1-drift.vec[2]), drift.vec[1]*x1*(drift.vec[2]-1)))
  diff <- c(diff.vec, -diff.vec)
  Sigma <- diag(diff)
  dt <- 2
  mu <- xt1 + t(Q) %*% xt1 * dt
  Var <- dt * Sigma %*% t(Sigma)
  k <- length(xt1)
  if (sum(log(svd(Var)$d)) == -Inf) {
    log.f.xt2.xt1 <- -1e+10
  } else {
    log.f.xt2.xt1 <- (-k/2 * log(2*pi) - 0.5*sum(log(svd(Var)$d)) -
                        0.5*(t(xt2 - mu) %*% ginv(Var) %*% (xt2 - mu)))
  }
  return(log.f.xt2.xt1)
}

# Define a function to compute the loss for parameter estimation.
loss <- function(par){
  n.sense <- result$n_senses
  n.Q <- 2
  n.Sigma <- 1
  drift.vec <- par[1:n.Q]
  diff.vec <- par[(n.Q+1):(n.Sigma+n.Q)]
  dat <- as.matrix(result$df)
  time.diff <- diff(dat[,1])
  loss <- sum(sapply(1:(length(time.diff)-1), function(i){
    xt1 <- dat[i,-1]
    xt2 <- dat[i+1,-1]
    x1 <- dat[i,2]
    x2 <- dat[i,3]
    loss <- log.cond.pdf(xt1, xt2, x1, x2, dt, drift.vec, diff.vec)
  })) + sum(dunif(drift.vec[1:2], max = 1000, min = 0, log = T))
  return(loss)
}


###################### sde parametric estimation #######################
# Define a function to compute the logarithm of the conditional probability density function with shared parameters.
log.cond.pdf.replica <- function(xt1, xt2, x1, x2, dt, drift.vec, diff.vec){
  # Shared drift matrix Q
  Q <- diag(c(drift.vec[1]*x2*(1-drift.vec[2]), drift.vec[1]*x1*(drift.vec[2]-1)))
  diff <- c(diff.vec, -diff.vec)
  Sigma <- diag(diff)
  mu <- xt1 + t(Q) %*% xt1 * dt
  Var <- dt * Sigma %*% t(Sigma)

  k <- length(xt1)
  if (sum(log(svd(Var)$d)) == -Inf) {
    log.f.xt2.xt1 <- -1e+10
  } else {
    log.f.xt2.xt1 <- (-k/2 * log(2*pi) - 0.5*sum(log(svd(Var)$d)) -
                        0.5*(t(xt2 - mu) %*% ginv(Var) %*% (xt2 - mu)))
  }
  return(log.f.xt2.xt1)
}

# Define a function to compute the loss for parameter estimation, modified for the replica model.
loss.replica <- function(par){
  n.sense <- result$n_senses
  n.Q <- 2
  n.Sigma <- 1
  drift.vec <- par[1:n.Q]  # Shared drift parameters for all replicas
  diff.vec <- par[(n.Q+1):(n.Sigma+n.Q)]  # Shared diffusion parameters for all replicas
  dat <- as.matrix(result$df)
  time.diff <- diff(dat[,1])

  loss <- sum(sapply(1:(length(time.diff)-1), function(i){
    xt1 <- dat[i,-1]
    xt2 <- dat[i+1,-1]
    x1 <- dat[i,2]
    x2 <- dat[i,3]
    loss <- log.cond.pdf.replica(xt1, xt2, x1, x2, dt, drift.vec, diff.vec)
  })) + sum(dunif(drift.vec[1:2], max = 1000, min = 0, log = T))

  return(loss)
}


# Example
result <- polysemous_data$entertain
set.seed(123)


# Initiate a Markov Chain Monte Carlo (MCMC) simulation to optimize the loss function over the parameter space.
# The initial parameter values are set uniformly at 0.01 with three parameters involved.
# The simulation is configured to run for 20,000 steps using a specified proposal kernel `kernel_ram()` to generate new parameter proposals.
sde_mcmc_entertain <- MCMC(fun = loss, initial = rep(0.01,3), nsteps = 20000, kernel = kernel_ram())

# Compute the log posterior probability density.
sde_lp_entertain <- apply(sde_mcmc_entertain, 1, loss)

# Conduct the Geweke diagnostic test on the last 2000 samples of the loss values computed from the MCMC simulation.
(geweke_test <- geweke.diag(tail(sde_lp_entertain, 2000)))

# Compute the model parameters by averaging the last 2000 samples from the MCMC output
sde_para_entertain <- colMeans(tail(sde_mcmc_entertain, 2000))


###################### sde solve #######################
library(Sim.DiffProc)

result <- polysemous_data$entertain
par_lv <- sde_para_entertain

set.seed(123)
mod1d <- snssde1d(
  drift = expression(par_lv[1] * x * (1 - x - par_lv[2] * (1 - x))),
  diffusion = expression(par_lv[3]),
  x0 = as.numeric(result$df[1, 2]),
  M = 10000,
  t0 = as.numeric(result$df[, 'year'][1]),
  T = as.numeric(result$df[, 'year'][nrow(result$df)]),
  Dt = 1,
  N = nrow(result$df) - 1,
  method = 'euler'
)
