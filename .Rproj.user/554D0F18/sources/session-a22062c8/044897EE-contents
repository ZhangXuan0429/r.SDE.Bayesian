setwd("D:/Fudan/Zhang Lab/language competition/diachronic_sense_modeling/data")
# load("it1ws5_data.Rdata")
setwd('/Users/zhangxuan/Desktop/ZhangLab/language competition/diachronic_sense_modeling/data')
# setwd('D:/Fudan/Zhang Lab/language competition/diachronic_sense_modeling/data')
# load('data_prop_fittingg_10.Rdata')
# load('sde_it1ws5.Rdata')
load('2025/sde_parmean_2k.Rdata')
load('2025/solve_it1ws5_v2_2w.Rdata')   # v2版本 3199词 迭代2w次
load('2025/sde_it1ws5_v2_2w_parmean.Rdata')
load('it1ws5_data_v2.Rdata')
polysemous_data <- it1ws5_data_v2[!(names(it1ws5_data_v2) %in% null_words)]
save(polysemous_data, file = 'D:/Fudan/Zhang Lab/language competition/diachronic_sense_modeling/data/2025/polysemous_data.Rdata')
########### ode/repli Geweke ############
load('2025/ode_it1ws5_v2_2w.Rdata')
load('2025/rep_it1ws5_v2_2w.Rdata')

ode_lp_list <- lapply(ode_it1ws5_v2_2w,
                 function(ls){ls$ode_mcmc_lp_2k})
ode_lp_list <- ode_lp_list[target_words]


rep_lp_list <- lapply(rep_it1ws5_v2_2w, function(ls){ls$rep_mcmc_lp_2k})
rep_lp_list <- rep_lp_list[target_words]


geweke_rep <- do.call(rbind, lapply(geweke_results, function(x) {
  data.frame(
    chain = x$chain_index,
    z_score = x$z_score,
    p_value = x$p_value,
    converged = x$converged
  )
}))

dim(subset(geweke_rep, converged == F))
save(geweke_ode, file = '2025/geweke_ode.Rdata')
save(geweke_rep, file = '2025/geweke_rep.Rdata')
# boxplot
dev.new()
converged_z <- subset(geweke_rep, converged == T)$z_score
boxplot(converged_z,col = "skyblue", border = "black", background = "white",
        axes = FALSE, frame.plot = TRUE, horizontal = FALSE,
        xlab = "", ylab = "", main = "", boxwex = 0.5 ,  outline =T)
axis(2, las = 1)  # 左侧纵轴，标签水平
abline(h = c(-1.96, 1.96), lty = 2, col = "red")
########### Geweke ############
load('2025/sde_mcmc_lp_2k_list.Rdata')
load('2025/sde_mcmc_lp_2k.Rdata')

library(coda)
sde_mcmc_lp_2k_list <- sde_mcmc_lp_2k_list[target_words]
sde_mcmc_lp_2k_list <- sde_mcmc_lp_2k[target_words]
sde_mcmc_lp_2k_list <- ode_lp_list
sde_mcmc_lp_2k_list <- rep_lp_list
geweke_results <- list()

for (i in seq_along(sde_mcmc_lp_2k_list)) {
  # 检查列表元素是否为NULL
  if (is.null(sde_mcmc_lp_2k_list[[i]])) {
    cat(sprintf("链 %d: 为NULL，跳过\n", i))
    next  # 跳过当前迭代
  }

  # 检查lpmanu列是否存在
  if (!"lpmanu" %in% names(sde_mcmc_lp_2k_list[[i]])) {
    cat(sprintf("链 %d: 缺少lpmanu列，跳过\n", i))
    next  # 跳过当前迭代
  }

  # 提取lpmanu列作为MCMC链
  chain <- sde_mcmc_lp_2k_list[[i]]$lpmanu

  # 检查提取的链是否为NULL
  if (is.null(chain)) {
    cat(sprintf("链 %d: lpmanu列为NULL，跳过\n", i))
    next  # 跳过当前迭代
  }

  # 检查链是否有足够的数据
  if (length(chain) < 100) {
    cat(sprintf("链 %d: 数据点太少 (%d个)，跳过\n", i, length(chain)))
    next  # 跳过当前迭代
  }

  # 将链转换为mcmc对象（coda包要求）
  chain_mcmc <- as.mcmc(chain)

  # 进行Geweke检验
  geweke_test <- geweke.diag(chain_mcmc)

  # 保存结果
  geweke_results[[i]] <- list(
    chain_index = i,
    z_score = geweke_test$z,
    p_value = 2 * pnorm(-abs(geweke_test$z)),  # 计算双侧p值
    converged = abs(geweke_test$z) < 1.96  # 使用1.96作为临界值（α=0.05）
  )

  cat(sprintf("链 %d: z = %.3f, p = %.4f, 收敛 = %s\n",
              i, geweke_test$z,
              2 * pnorm(-abs(geweke_test$z)),
              ifelse(abs(geweke_test$z) < 1.96, "是", "否")))
}

# geweke result
geweke_360no <- do.call(rbind, lapply(geweke_results, function(x) {
  data.frame(
    chain = x$chain_index,
    z_score = x$z_score,
    p_value = x$p_value,
    converged = x$converged
  )
}))

geweke_328no <- do.call(rbind, lapply(geweke_results, function(x) {
  data.frame(
    chain = x$chain_index,
    z_score = x$z_score,
    p_value = x$p_value,
    converged = x$converged
  )
}))

dim(subset(geweke_360no, converged == F))

# boxplot
dev.new()

pdf_path <- "/Users/zhangxuan/Desktop/ZhangLab/language competition/diachronic_sense_modeling/data/2025/Geweke_boxplot.pdf"

# 打开PDF设备，设置图形参数
pdf(file = pdf_path,
    width = 8,  # 宽度8英寸
    height = 6, # 高度6英寸
    paper = "a4") # 使用A4纸大小

converged_z <- subset(geweke_360no, converged == T)$z_score
boxplot(converged_z,col = "skyblue", border = "black", background = "white",
  axes = FALSE, frame.plot = TRUE, horizontal = FALSE,
  xlab = "", ylab = "", main = "", boxwex = 0.5 )

axis(2, las = 1)  # 左侧纵轴，标签水平
abline(h = c(-1.96, 1.96), lty = 2, col = "red")
dev.off()

# 确认保存
cat(sprintf("PDF已保存到：%s\n", pdf_path))

############### 生成rda数据 #############
it1ws5 <- read.csv('C:/Users/84254/Desktop/投稿/sense_ratio_it1ws5.csv')
it1ws5 <- read.csv('C:/Users/84254/Desktop/投稿/sense_ratio_it1ws5_v2.csv')
it1ws5_data_new <- make_polysemous_data(it1ws5)


## Required packages
library(dplyr)
library(purrr)
library(tidyr)
library(stringr)
library(tibble)
make_polysemous_data <- function(df) {
  # deps
  if (!requireNamespace("tidyverse", quietly = TRUE)) {
    stop("Please install.packages('tidyverse') first.")
  }
  library(tidyverse)

  # helper: parse x/y cell into numeric vector (supports list, numeric, "[...]" char)
  parse_vec <- function(v, as_integer = FALSE) {
    if (is.list(v)) v <- v[[1]]
    if (is.numeric(v)) out <- v else {
      # NOTE: generic decimal-only regex kept for x and other fields
      nums <- stringr::str_extract_all(as.character(v), "-?\\d+\\.?\\d*")[[1]]
      out <- as.numeric(nums)
    }
    if (as_integer) out <- as.integer(out)
    out
  }

  # ---- NEW: dedicated parser for y that preserves scientific notation ----
  parse_y_vec <- function(v) {                                                # <-- CHANGED (new helper)
    if (is.list(v)) v <- v[[1]]                                              # <-- CHANGED
    ch <- as.character(v)                                                    # <-- CHANGED (force character first)
    # exponent-aware pattern: optional sign, digits, optional dot, digits, optional exponent
    nums <- stringr::str_extract_all(ch, "-?\\d*\\.?\\d+(?:[eE][+-]?\\d+)?")[[1]]  # <-- CHANGED
    as.numeric(nums)                                                         # <-- CHANGED
  }
  # -----------------------------------------------------------------------

  # 1) build polysemous_data with wide df per word
  res <- df %>%
    dplyr::group_split(.data$word, .keep = TRUE) %>%
    purrr::map(function(dfw) {
      w <- dfw$word[1]

      senses_list <- split(dfw, dfw$sense_id) %>%
        purrr::imap(function(sdf, sid) list(
          definition = sdf$definition[[1]],
          x = parse_vec(sdf$x, as_integer = TRUE),
          y = parse_y_vec(sdf$y)                                            # <-- CHANGED (use y-specific parser)
        ))

      all_years <- sort(unique(unlist(purrr::map(senses_list, "x"))))
      wide <- tibble::tibble(year = all_years)
      for (sid in names(senses_list)) {
        xi <- senses_list[[sid]]$x
        yi <- senses_list[[sid]]$y
        v <- rep(NA_real_, length(all_years))
        v[match(xi, all_years)] <- yi
        wide[[sid]] <- v
      }

      list(word = w, senses = senses_list, n_senses = length(senses_list), df = wide)
    }) %>%
    purrr::set_names(purrr::map_chr(., "word"))

  # 2) replace each df with 3 columns: year / sense1 (max col of first row) / sense2 = 1 - sense1
  res <- purrr::map(res, function(item) {
    dfw <- item$df
    stopifnot("year" %in% names(dfw))
    sense_cols <- setdiff(names(dfw), "year")

    # choose column with max value in the first row (ignore NA; if all NA, fallback to first sense col)
    row1 <- suppressWarnings(as.numeric(dfw[1, sense_cols]))
    if (all(is.na(row1))) {
      max_col <- sense_cols[1]
    } else {
      row1[is.na(row1)] <- -Inf
      max_col <- sense_cols[which.max(row1)]
    }

    item$df <- tibble::tibble(
      year   = dfw$year,
      sense1 = as.numeric(dfw[[max_col]]),
      sense2 = 1 - as.numeric(dfw[[max_col]])
    )
    item
  })
  res
}

it1ws5_data_v2 <- it1ws5_data_new
save(it1ws5_data_v2, file = 'it1ws5_data_v2.Rdata')

it1ws5_data <- it1ws5_data_new

############### 找出开始sense ratio为0的 #############
# 假设 it1ws5_data 是一个命名的列表
result_names <- names(it1ws5_data)[
  sapply(it1ws5_data, function(x) {
    # 确保存在 df 且是数据框，并且至少有一行两列
    if (!is.null(x$df) && is.data.frame(x$df) && nrow(x$df) >= 1 && ncol(x$df) >= 2) {
      return(x$df[1, 2] == 0)
    } else {
      return(FALSE)
    }
  })
]

# 输出满足条件的子列表名称
print(result_names)
(n_senses_values <- sapply(result_names, function(name) {
  it1ws5_data[[name]]$n_senses
}))


it1ws5_data$acquire$senses$acquire_1_verb_2$y+
  it1ws5_data$acquire$senses$acquire_1_verb_1$y
federal_1_adjective_2$y
federal_1_adjective_1$y




# 数值比较的容差
tol <- 1e-8

# 遍历每个子列表
bad_items <- names(it1ws5_data)[
  sapply(it1ws5_data, function(item) {
    # 检查senses是否存在且长度>=2
    if (is.null(item$senses) || length(item$senses) == 0) {
      return(TRUE)  # 没有senses视作异常
    }

    # 提取所有y向量
    y_list <- lapply(item$senses, function(s) s$y)

    # 保证所有y长度一致并且都是数值
    if (any(sapply(y_list, is.null)) || length(unique(sapply(y_list, length))) != 1) {
      return(TRUE)
    }

    # 按元素求和
    y_sum <- Reduce("+", y_list)

    # 判断是否全部≈1
    if (all(abs(y_sum - 1) <= tol)) {
      return(FALSE)
    } else {
      return(TRUE)
    }
  })
]

# 输出结果
print(bad_items)
# 将 bad_items 转换为数据框
bad_items_df <- data.frame(word = result_names, stringsAsFactors = FALSE)

# 写出为 CSV 文件
write.csv(bad_items_df, file = "bad_items.csv", row.names = FALSE)

# 提示输出路径
cat("已成功保存为文件:", normalizePath("bad_items.csv"), "\n")



# 计算 bad_items 中每个词条的 y 向量求和结果
bad_items_y_sums <- lapply(bad_items, function(name) {
  item <- it1ws5_data[[name]]
  y_list <- lapply(item$senses, function(s) s$y)
  y_sum <- Reduce("+", y_list)
  return(y_sum)
})

# 给结果加上名称
names(bad_items_y_sums) <- bad_items

# 输出结果（可简略或完整查看）
print(bad_items_y_sums)

# 若只想看不为1的位置：
# lapply(bad_items_y_sums, function(v) which(abs(v - 1) > 1e-8))




(n_senses_values <- sapply(bad_items, function(name) {
  it1ws5_data[[name]]$n_senses
}))

# 打印结果
print(n_senses_values)


intersect(bad_items,result_names )







############### sde plot #############
# industry welfare engine dramatically
# episode
# result <- polysemous_data$dramatically
# (par_lv <- sde_it1ws5_parmean$dramatically)

result <- it1ws5_data_v2$youth
(par_lv <- sde_it1ws5_v2_2w_parmean$youth)
sde.res <- solve_it1ws5_v2_2w$youth

# sde
set.seed(123)
mod1d <- snssde1d(drift=expression(par_lv[1]*x*(1-x-par_lv[2]*(1-x))),
                  diffusion=expression(par_lv[3]),
                  x0=as.numeric(result$df[1,2]),
                  M=10000,
                  t0=result$df$year[1],T=result$df$year[length(result$df$year)],
                  Dt=2,
                  N=length(result$df$year)-1,
                  method = 'euler')

is_in_interval_01 <- function(column) {all(column >= 0 & column <= 1)}
selected_columns <- apply(mod1d$X, 2, is_in_interval_01)
mod1d_01 <- mod1d$X[, selected_columns]
dim(mod1d_01)
# if (dim(mod1d_01)[2] > 10000) {
#   mod1d_01 <- mod1d_01[, 1:10000]
# }
mod1d$X <- mod1d_01
mse.sde <- apply(mod1d$X, 2, function(x) mean((unlist(result$df[,2]) - x)^2))
mse.index.sde <- which.min(mse.sde)
(mse.min.sde <- mse.sde[mse.index.sde])
sde.res <- mod1d$X[,mse.index.sde]


# X  <- as.matrix(mod1d$X)
#
# # CI
# # 计算每个时间点（每行）的均值和标准差
# mean_per_time <- apply(X, 1, mean, na.rm = TRUE)
# sd_per_time   <- apply(X, 1, sd,   na.rm = TRUE)
#
# # 95%置信区间：均值 ± 1.96 * 标准差
# ci_lower <- mean_per_time - 1.96 * sd_per_time
# ci_upper <- mean_per_time + 1.96 * sd_per_time
#
# ci_lower <- sde.res$sde - 1.96 *sd_per_time
# ci_upper <- sde.res$sde + 1.96 *sd_per_time
# # 整理成数据框，方便绘图或输出
# ci_df <- data.frame(
#   time  = result$df[, 1],   # 如果 result$df 第一列是时间
#   mean  = mean_per_time,
#   lower = ci_lower,
#   upper = ci_upper
# )
#
# sde_lower_quantile <- ci_df$lower # monopoly
# sde_upper_quantile <- ci_df$upper  # monopoly
# polygon(c(result$df$year, rev(result$df$year)), c(sde_lower_quantile, rev(sde_upper_quantile)),
#         border = NA,
#         # rgb (红,绿,蓝,透明度) [0,1]
#         col = rgb(0.678, 0.847, 0.902, 0.5)  # c('skyblue3','salmon')
#         # col = rgb(0.1, 0.8, 0.3, 0.2)        #  c('#1b7c3d','#f16c00')
#         # col = rgb(0.97, .3, 0.32, .16)       # c(rgb(0.7, .4, 0.4, .7),'#8f85c2')
# )
# polygon(c(result$df$year, rev(result$df$year)), c(1-sde_lower_quantile, rev(1-sde_upper_quantile)),
#         border = NA,
#         col =rgb(0.945, 0.827, 0.780, 0.8)  # c('skyblue3','salmon')
#         # col = rgb(1, 1, 0.7, 0.6)            #  c('#1b7c3d','#f16c00')
#         # col =rgb(0.62, .01, 0.42, .18)      # c(rgb(0.7, .4, 0.4, .7),'#8f85c2')
# )

############### 直接plot sde分解 ###############
result <- it1ws5_data_v2$fairly
(par_lv <- sde_it1ws5_v2_2w_parmean$fairly)
sde.res <- solve_it1ws5_v2_2w$fairly

# sde
# dev.new()
matplot(result$df$year, data.frame(sde.res$sde,1-sde.res$sde), type = "l", ylim = c(0,1), lwd = 3, lty = 1,
        xlab = "Year", ylab = 'Proportion',
        # main = "SDE-Fitted Sense Proportions\ndt = 1",
        # xlab = "", ylab = '', main = "",
        # col = c('skyblue3','salmon')
        col = c('#B37666','#3A6C6E')
        # col = c(rgb(0.7, .4, 0.4, .7),'#8f85c2')
)

points(result$df$year, result$df$sense2, pch=4, lwd = 2.5,
       # col='skyblue3'
       col='#8C6239'
       # col = rgb(0.7, .4, 0.4, .7)
)
points(result$df$year, result$df$sense1, pch=1, lwd = 2.5,
       # col='salmon'
       col='#A16F82'
       # col = '#8f85c2'
)
selected_names <- names(it1ws5_data_v2)[
  names(it1ws5_data_v2) %in% wil_stableword_all &
    sapply(it1ws5_data_v2, function(x) x$n_senses == 2)
]
# advantage
matplot(result$df$year,
        result$df$sense1, pch=16, lwd = 2.5,
        ylim = c(0,1),
        xlab = "", ylab = '', main = "", lty = 1,
        col='salmon')

points(result$df$year, result$df$sense2, pch=20, lwd = 2.5,
       col='skyblue3'
       # col='#3A6C6E'
       # col = '#8f85c2'
)
# agency
matplot(result$df$year,
        result$df$sense1, pch=16, lwd = 2.5,
        ylim = c(0,1),
        xlab = "", ylab = '', main = "", lty = 1,
        col='salmon')

points(result$df$year, result$df$sense2, pch=20, lwd = 2.5,
       col='skyblue3'
       # col='#3A6C6E'
       # col = '#8f85c2'
)

# dissolve
matplot(result$df$year,
        result$senses$dissolve_1_noun_1$y, pch=16, lwd = 2.5,
        ylim = c(0,1),
        xlab = "", ylab = '', main = "", lty = 1,
        col='#85b760')

points(result$df$year, result$senses$dissolve_1_verb_1$y, pch=20, lwd = 2.5,
       col='salmon'
       # col='#3A6C6E'
       # col = '#8f85c2'
)
points(result$df$year, result$senses$dissolve_1_verb_2$y, pch=16, lwd = 2.5,
       col='skyblue3'
       # col='#B37666'
       # col = rgb(0.7, .4, 0.4, .7)
)
# ode
matplot(result$df$year, data.frame(sde.res$ode,1-sde.res$ode), type = "l",
        xlab = "Year", ylab = 'Proportion',
        ylim = c(0,1),
        lwd = 3, lty = 1,
        # col = c('skyblue3','salmon')
        col=c('#A16F82','#8C6239')
        # col = c('#1b7c3d','#f16c00')
)

# sigmadw
matplot(result$df$year, data.frame(sde.res$sigmadw,-sde.res$sigmadw), type = "l",
        xlab = "Year", ylab = 'Proportion',
        ylim = c(-.4,.4),
        lwd = 3, lty = 1,
        col = c('#A16F82','#8C6239')
        # col = c('#1b7c3d','#f16c00')
)


################# 词义占比散点图############
result <- it1ws5_data_v2$fighter

matplot(result$df$year, data.frame(2:(length(result$df$year)+1),1-2:(length(result$df$year)+1)), type = "l",
        xlab = "", ylab = '',
        main = "",
        ylim = c(0,1),
        lwd = 3, lty = 1)

# 看dominant sense排序
sapply(result$senses, function(sense) {
  sense$y[1]
}) %>% order(decreasing = T)

# ---a--- #
# fighter
points(result$df$year, result$df$sense1, col='salmon', pch=4, lwd = 3)
points(result$df$year, result$df$sense2, col='skyblue3', pch=5, lwd = 3)


# tank
points(result$df$year, result$senses[[1]]$y, col='salmon', pch=4, lwd = 3)
points(result$df$year, result$senses[[4]]$y, col='skyblue3', pch=5, lwd = 3)
points(result$df$year, result$senses[[2]]$y, col='#ffcf49', pch=3, lwd = 3)
points(result$df$year, result$senses[[6]]$y, col=rgb(0.7, .4, 0.4, .7), pch=8, lwd = 3)
points(result$df$year, result$senses[[5]]$y, col='#8f85c2', pch=6, lwd = 3)
points(result$df$year, result$senses[[8]]$y, col='grey', pch=2, lwd = 3)
points(result$df$year, result$senses[[3]]$y, col='#1b7c3d', pch=2, lwd = 3)
points(result$df$year, result$senses[[7]]$y, col='pink2', pch=2, lwd = 3)

# dialogue
points(result$df$year, result$df$sense1, col='salmon', pch=4, lwd = 3)
points(result$df$year, result$df$sense2, col='skyblue3', pch=5, lwd = 3)

# peaceful
points(result$df$year, result$df$sense2, col='salmon', pch=4, lwd = 3)
points(result$df$year, result$df$sense1, col='skyblue3', pch=5, lwd = 3)

# ---b--- #
# company
points(result$df$year, result$senses[[2]]$y, col='salmon', pch=4, lwd = 3)
points(result$df$year, result$senses[[4]]$y, col='skyblue3', pch=5, lwd = 3)
points(result$df$year, result$senses[[3]]$y, col='#ffcf49', pch=3, lwd = 3)
points(result$df$year, result$senses[[1]]$y, col=rgb(0.7, .4, 0.4, .7), pch=8, lwd = 3)
points(result$df$year, result$senses[[5]]$y, col='#8f85c2', pch=6, lwd = 3)

# staff
points(result$df$year, result$senses[[3]]$y, col='salmon', pch=4, lwd = 3)
points(result$df$year, result$senses[[2]]$y, col='skyblue3', pch=5, lwd = 3)
points(result$df$year, result$senses[[1]]$y, col='#ffcf49', pch=3, lwd = 3)
points(result$df$year, result$senses[[4]]$y, col=rgb(0.7, .4, 0.4, .7), pch=8, lwd = 3)
points(result$df$year, result$senses[[5]]$y, col='#8f85c2', pch=6, lwd = 3)
points(result$df$year, result$senses[[6]]$y, col='grey', pch=2, lwd = 3)

# bus
points(result$df$year, result$senses[[4]]$y, col='salmon', pch=4, lwd = 3)
points(result$df$year, result$senses[[3]]$y, col='skyblue3', pch=5, lwd = 3)
points(result$df$year, result$senses[[1]]$y, col='#ffcf49', pch=3, lwd = 3)
points(result$df$year, result$senses[[2]]$y, col=rgb(0.7, .4, 0.4, .7), pch=8, lwd = 3)

# engine
points(result$df$year, result$senses[[2]]$y, col='salmon', pch=4, lwd = 3)
points(result$df$year, result$senses[[1]]$y, col='skyblue3', pch=5, lwd = 3)

# ---c--- #
# scarce
points(result$df$year, result$senses[[2]]$y, col='salmon', pch=4, lwd = 3)
points(result$df$year, result$senses[[1]]$y, col='skyblue3', pch=5, lwd = 3)

# retire
points(result$df$year, result$senses[[2]]$y, col='salmon', pch=4, lwd = 3)
points(result$df$year, result$senses[[1]]$y, col='skyblue3', pch=5, lwd = 3)
points(result$df$year, result$senses[[4]]$y, col='#ffcf49', pch=3, lwd = 3)
points(result$df$year, result$senses[[5]]$y, col=rgb(0.7, .4, 0.4, .7), pch=8, lwd = 3)
points(result$df$year, result$senses[[3]]$y, col='#8f85c2', pch=6, lwd = 3)

# channel
points(result$df$year, result$senses[[3]]$y, col='salmon', pch=4, lwd = 3)
points(result$df$year, result$senses[[1]]$y, col='skyblue3', pch=5, lwd = 3)
points(result$df$year, result$senses[[2]]$y, col='#ffcf49', pch=3, lwd = 3)
points(result$df$year, result$senses[[4]]$y, col=rgb(0.7, .4, 0.4, .7), pch=8, lwd = 3)
points(result$df$year, result$senses[[7]]$y, col='#8f85c2', pch=6, lwd = 3)
points(result$df$year, result$senses[[5]]$y, col='grey', pch=2, lwd = 3)
points(result$df$year, result$senses[[6]]$y, col='#1b7c3d', pch=2, lwd = 3)
points(result$df$year, result$senses[[8]]$y, col='pink2', pch=2, lwd = 3)

# faculty
points(result$df$year, result$senses[[1]]$y, col='salmon', pch=4, lwd = 3)
points(result$df$year, result$senses[[3]]$y, col='skyblue3', pch=5, lwd = 3)
points(result$df$year, result$senses[[2]]$y, col='#ffcf49', pch=3, lwd = 3)









####### 其他词占比 ######
result <- it1ws5_data_v2$expert

matplot(result$df$year, data.frame(2:(length(result$df$year)+1),1-2:(length(result$df$year)+1)), type = "l",
        xlab = "", ylab = '',
        main = "",
        ylim = c(0,1),
        lwd = 3, lty = 1)

# 看dominant sense排序
sapply(result$senses, function(sense) {
  sense$y[1]
}) %>% order(decreasing = T)


# industry 好!
points(result$df$year, result$senses[[2]]$y, col='salmon', pch=4, lwd = 3)
points(result$df$year, result$senses[[1]]$y, col='skyblue3', pch=5, lwd = 3)
matplot(seq(1820,2010,10),Freq_to_vec$industry/Freq[,'tokens']*100000, type = "l", lwd = 3, lty = 1,
        ylim = c(0,100),
        xlab = "", ylab = '', main = "",  col = '#1F783B')
matplot(seq(1820,2010,10),Freq_to_vec$industry, ylim = c(-.5,.5),
        xlab = "", ylab = '', main = "",lwd = 3, lty = 1, col = 'white')
lines(solve_it1ws5_v2_2w$industry$year,solve_it1ws5_v2_2w$industry$sigmadw,
      lwd = 3, lty = 1, col = '#E56A27')

# expert
points(result$df$year, result$senses[[1]]$y, col='salmon', pch=4, lwd = 3)
points(result$df$year, result$senses[[2]]$y, col='skyblue3', pch=5, lwd = 3)
points(result$df$year, result$senses[[2]]$y, col='#ffcf49', pch=3, lwd = 3)
points(result$df$year, result$senses[[3]]$y, col=rgb(0.7, .4, 0.4, .7), pch=8, lwd = 3)
points(result$df$year, result$senses[[5]]$y, col='#8f85c2', pch=6, lwd = 3)
points(result$df$year, result$senses[[6]]$y, col='grey', pch=2, lwd = 3)
matplot(seq(1870,2010,10),(Freq_to_vec$expert/Freq[,'tokens']*100000)[-c(1:5)], type = "l", lwd = 3, lty = 1,
        ylim = c(0,30),
        xlab = "", ylab = '', main = "",  col = '#1F783B')
matplot(seq(1870,2010,10),Freq_to_vec$expert[-c(1:5)], ylim = c(-.4,.4),
        xlab = "", ylab = '', main = "",lwd = 3, lty = 1, col = 'white')
lines(solve_it1ws5_v2_2w$expert$year,-solve_it1ws5_v2_2w$expert$sigmadw,
      lwd = 3, lty = 1, col = '#E56A27')
################## Fig.1 散点图##################
load("2025/ws5normed_word_freq.Rdata")
(resu <- it1ws5_data_v2$divine)
freq <- ws5normed_word_freq$divine

## 1) 取出四条序列
keys <- c("faculty_1_noun_1", "faculty_1_noun_2",
          "faculty_1_noun_3")
keys <- c("professor_1_noun_1",
          "professor_1_noun_2")
keys <- c("expert_1_adjective_1",
          "expert_1_noun_1")
keys <- c("coach_1_noun_3","coach_1_noun_1",
"coach_1_noun_2","coach_1_adverb_1",
"coach_1_verb_1","coach_2_noun_1",
          "coach_2_verb_1")

# 5d freq明显衰退
keys <- c("heaven_1_noun_1", "heaven_1_noun_2",
          "heaven_1_noun_3")
# keys <- c("hell_1_noun_1",
#           "hell_1_exclamation_1")
keys <- c("divine_1_adjective_1","divine_1_adjective_2",
          "divine_1_noun_1","divine_1_noun_2",
          "divine_2_verb_1","divine_2_verb_2")
keys <- c("eternal_1_adjective_1", "eternal_1_adjective_3",
          "eternal_1_adjective_2")
keys <- c("angel_1_noun_1","angel_1_noun_2",
          "angel_1_noun_6","angel_1_noun_5",
          "angel_1_noun_4",'angel_1_noun_3')
keys <- c("evil_1_noun_1",
          "evil_1_adjective_1")

# 5d freq明显上升
keys <- c("couple_1_noun_3","couple_1_noun_2",
          "couple_1_verb_1","couple_1_noun_1",
          "couple_1_verb_2")
keys <- c("parent_1_noun_1",
          "parent_1_verb_1")
keys <- c("partner_1_noun_1","partner_1_noun_2",
          "partner_1_noun_3","partner_2_noun_1",
          "partner_1_verb_1")
keys <- c("kid_1_noun_2","kid_1_noun_1",
          "kid_2_verb_1","kid_1_verb_1",
          "kid_3_noun_1")
keys <- c("baby_1_noun_1","baby_1_noun_2",
          "baby_1_adjective_1","baby_1_verb_1")
x_list <- lapply(keys, function(k) resu$senses[[k]]$x)
y_list <- lapply(keys, function(k) resu$senses[[k]]$y)

## 2) 组装成矩阵（假设四组 x 完全一致；你给的例子里确实一致）
x <- x_list[[1]]
y_mat <- do.call(cbind, y_list)
colnames(y_mat) <- keys

## 3) 用 matplot 画散点
cols <- c('#D17977','#699FC3', "#A16F82", "#ffcf49",
          '#E56A27','#3A6C6E','#8f85c2','pink2')  # 四种颜色，可自行改
pchs <- c(16, 16, 16, 16)
# dev.new()
# setwd('D:/Fudan/Zhang Lab/SDE投稿/Fig.PDF')

matplot(x, y_mat,
        type = "p", pch = pchs, col = cols,
        xlab = "Year", ylab = "Sense proportion",
        main = resu$word,ylim = c(0,1))

matplot(x, freq$freq,
        type = "p", pch = 4, col = '#84B65F',
        # ylim = c(0,700),
        # ylim = c(0,200),
        xlab = "Year", ylab = "Sense proportion")


points(x, result$df$sense1, pch=16, lwd = 2.5,
       col='#84B65F'
       # col='#B37666'
       # col = rgb(0.7, .4, 0.4, .7)
)
################## Fig.5 棒棒糖##################
library(readxl)
library(dplyr)
library(tidyr)
library(ggplot2)
library(scales)
# ---------- 1) Read data ----------
xlsx_path <- "D:/Fudan/Zhang Lab/SDE投稿/2025汇报/2_change_data_byGroup.xlsx"
df <- read_excel(xlsx_path) %>%
  transmute(
    topic = as.character(topic),
    word  = as.character(word),
    group = tolower(as.character(group)),
    value = as.numeric(value)
  )

# ---------- 2) Topic order: semantic median (largest on TOP) ----------
# 关键：用“升序 levels”，coord_flip 后最大值会出现在最上面
sem_sum <- df %>%
  filter(group == "semantic") %>%
  group_by(topic) %>%
  # summarise(med = median(value, na.rm = TRUE), .groups = "drop") %>%
  summarise(med = mean(value, na.rm = TRUE), .groups = "drop") %>%
  arrange(med)  # 升序：最大值最后 -> 翻转坐标后在最上

topic_levels <- sem_sum$topic

sem_sum <- sem_sum %>%
  mutate(topic = factor(topic, levels = topic_levels))

# ---------- 3) Freq summary with SAME topic order ----------
freq_sum <- df %>%
  filter(group == "freq") %>%
  group_by(topic) %>%
  # summarise(med = median(value, na.rm = TRUE), .groups = "drop") %>%
  summarise(med = mean(value, na.rm = TRUE), .groups = "drop") %>%
  right_join(tibble(topic = topic_levels), by = "topic") %>%   # 强制补齐所有 topic
  mutate(med = replace_na(med, 0),
         topic = factor(topic, levels = topic_levels))

# ---------- 4) Energetic palette: same topic same color ----------
pal_topic <- setNames(
  hue_pal(l = 65, c = 120)(length(topic_levels)),  # 偏“元气”的高饱和离散色
  topic_levels
)
# pal_topic <- setNames(rainbow(length(topic_levels), s = 0.9, v = 0.9), topic_levels)


# ---------- 5) Lollipop plots (stem = grey, candy = colored by topic) ----------

p_sem <- ggplot(sem_sum, aes(x = topic, y = med)) +
  # 棒：统一灰色
  geom_segment(aes(xend = topic, y = 0, yend = med),
               linewidth = 1.2, lineend = "round",
               color = "grey75", alpha = 0.9) +
  # 糖：按 topic 着色
  geom_point(aes(color = topic), size = 4.2) +
  coord_flip() +
  scale_color_manual(values = pal_topic) +
  labs(
    # title = "Semantic group: Topic medians (lollipop)",
    x = NULL, y = "Mean value", color = "Topic"
  ) +
  theme_minimal(base_size = 12) +
  theme(
    panel.grid.major.y = element_blank(),
    panel.grid.minor = element_blank(),
    plot.title = element_text(face = "bold")
  )

p_freq <- ggplot(freq_sum, aes(x = topic, y = med)) +
  geom_segment(aes(xend = topic, y = 0, yend = med),
               linewidth = 1.2, lineend = "round",
               color = "grey75", alpha = 0.9) +
  geom_point(aes(color = topic), size = 4.2) +
  coord_flip() +
  scale_color_manual(values = pal_topic) +
  labs(
    # title = "Freq group: Topic medians (same topic order & colors)",
    x = NULL, y = "Mean value", color = "Topic"
  ) +
  theme_minimal(base_size = 12) +
  theme(
    panel.grid.major.y = element_blank(),
    panel.grid.minor = element_blank(),
    plot.title = element_text(face = "bold")
  )

print(p_sem)
print(p_freq)

################## Fig.5 词频曲线 ##################

# 需要绘制的词
words <- c("faculty", "professor", "expert", "coach")
words <- c("heaven", "divine", "eternal", "angel","evil")
words <- c("couple","parent","partner","kid","baby")
# 把列表里每个词的数据取出来并合并成长表
df_long <- bind_rows(lapply(words, function(w) {
  x <- ws5normed_word_freq[[w]]
  if (is.null(x)) stop(paste("ws5normed_word_freq 中找不到：", w))
  x %>%
    transmute(year = as.integer(year),
              freq = as.numeric(freq),
              word = w)
}))

# 横坐标取四者year的并集（最长覆盖）
year_range <- range(df_long$year, na.rm = TRUE)
cols <- c('#D17977','#699FC3', "#A16F82", "#ffcf49",
          '#E56A27','#3A6C6E','#8f85c2','pink2')
p <- ggplot(df_long, aes(x = year, y = freq, color = word)) +
  geom_line(linewidth = 1) +
  scale_x_continuous(limits = year_range, breaks = pretty(year_range, n = 8)) +
  scale_y_continuous(limits = c(0, 400), breaks = seq(0, 400, by = 50), expand = c(0, 0)) +
  scale_color_manual(values = setNames(cols[1:length(words)], words)) +
  labs(x = "Year", y = "Normalized frequency (per million)", color = NULL) +
  theme_classic(base_size = 14) +
  theme(legend.position = "top")

print(p)
################## Fig.5 词义气泡 ##################
library(forcats)

# 1) 读入数据（把路径改成你的文件）
df <- read_excel("D:/Fudan/Zhang Lab/SDE投稿/2025汇报/1_change_data_byWord.xlsx", sheet = 1) %>%
  mutate(
    word = as.character(word),
    max_sense_ratio_std = as.numeric(max_sense_ratio_std),
    freq_CV = as.numeric(freq_CV)
  )

# 2) 三类词表
edu_words <- c("faculty", "professor", "expert", "coach")
rel_words <- c("heaven", "divine", "eternal", "angel", "evil")
kin_words <- c("couple", "parent", "partner", "kid", "baby")

word_map <- tibble::tibble(
  word = c(edu_words, rel_words, kin_words),
  class = c(rep("Education", length(edu_words)),
            rep("Religion",  length(rel_words)),
            rep("Kinship",   length(kin_words)))
)

# 3) 筛选 + 排序（每个类别内按 max_sense_ratio_std 从大到小）
df_sel <- df %>%
  inner_join(word_map, by = "word") %>%
  group_by(class) %>%
  mutate(word = fct_reorder(word, max_sense_ratio_std, .desc = TRUE)) %>%
  ungroup() %>%
  mutate(class = factor(class, levels = c("Education", "Religion", "Kinship")))

# 4) 画气泡图：y=max_sense_ratio_std；气泡大小=freq_mean（可按需替换）
p <- ggplot(df_sel, aes(x = word, y = max_sense_ratio_std)) +
  geom_point(
    aes(size = freq_CV, fill = class),
    shape = 21, color = "grey30", alpha = 0.85, stroke = 0.4
  ) +
  facet_grid(class ~ ., scales = "free_x", space = "free_x") +
  scale_fill_manual(values = c(
    Education = "#3B82F6",
    Religion  = "#F59E0B",
    Kinship   = "#10B981"
  )) +
  scale_size(range = c(3, 10), name = "freq_CV") +
  labs(x = NULL, y = "max_sense_ratio_std") +
  theme_classic(base_size = 14) +
  theme(
    legend.position = "right",
    strip.background = element_blank(),
    strip.text = element_text(face = "bold"),
    axis.text.x = element_text(angle = 0, vjust = 0.5)
  )

print(p)

# 如果想把气泡大小改为 freq_CV：
# 把 aes(size = freq_mean) 改成 aes(size = freq_CV)

################## Fig.5 热图 ##################
library(stringr)
library(purrr)

x <- df$freq_CV
y <- df$max_sense_ratio_std
ok <- is.finite(as.numeric(x)) & is.finite(as.numeric(y))
cor(as.numeric(x)[ok], as.numeric(y)[ok])# 不拆topic

x2 <- df_long$freq_CV
y2 <- df_long$max_sense_ratio_std
ok2 <- is.finite(x2) & is.finite(y2)
cor(x2[ok2], y2[ok2])                 # 拆topic

# ---- 0) Input ----
xlsx_path <- "D:/Fudan/Zhang Lab/SDE投稿/2025汇报/1_change_data_byWord.xlsx"
sheet_name <- 1                 # or "Sheet1"

df <- read_xlsx(xlsx_path, sheet = sheet_name)

# ---- 1) Expand topics: duplicate rows for multi-topic words ----
df_long <- df %>%
  mutate(topic = as.character(topic)) %>%
  separate_rows(topic, sep = "\\s*;\\s*") %>%
  mutate(topic = str_trim(topic)) %>%
  filter(!is.na(topic), topic != "")

# ---- 2) Columns to correlate (fixed order required by you) ----
vars <- c("freq_CV", "max_sense_ratio_std", "r1", "sigma1", "c")

# Ensure numeric
df_long <- df_long %>%
  mutate(across(all_of(vars), ~ suppressWarnings(as.numeric(.x))))

# ---- 3) Helper: compute r and p matrices (pairwise complete) ----
corr_with_p <- function(dat, vars, method = "pearson") {
  X <- dat %>% select(all_of(vars))
  r_mat <- cor(X, use = "pairwise.complete.obs", method = method)

  p_mat <- matrix(NA_real_, nrow = length(vars), ncol = length(vars),
                  dimnames = list(vars, vars))
  n_mat <- matrix(NA_integer_, nrow = length(vars), ncol = length(vars),
                  dimnames = list(vars, vars))

  for (i in seq_along(vars)) {
    for (j in seq_along(vars)) {
      xi <- X[[i]]
      xj <- X[[j]]
      ok <- is.finite(xi) & is.finite(xj)
      n_mat[i, j] <- sum(ok)
      if (i == j) {
        p_mat[i, j] <- 0
      } else if (sum(ok) >= 3) {
        ct <- suppressWarnings(cor.test(xi[ok], xj[ok], method = method))
        p_mat[i, j] <- ct$p.value
      } else {
        p_mat[i, j] <- NA_real_
      }
    }
  }

  list(r = r_mat, p = p_mat, n = n_mat)
}

# ---- 4) Helper: stars ----
sig_stars <- function(p) {
  dplyr::case_when(
    is.na(p) ~ "",
    p < 0.001 ~ "***",
    p < 0.01  ~ "**",
    p < 0.05  ~ "*",
    TRUE      ~ ""
  )
}

# ---- 5) Heatmap with FIXED variable order + 3-decimal r ----
plot_corr_heatmap <- function(r_mat, p_mat, vars_order, title = NULL) {
  v_ord <- vars_order

  r_long <- as.data.frame(as.table(r_mat)) %>%
    rename(var1 = Var1, var2 = Var2, r = Freq) %>%
    mutate(
      var1 = factor(var1, levels = v_ord),
      var2 = factor(var2, levels = v_ord)
    )

  p_long <- as.data.frame(as.table(p_mat)) %>%
    rename(var1 = Var1, var2 = Var2, p = Freq) %>%
    mutate(
      var1 = factor(var1, levels = v_ord),
      var2 = factor(var2, levels = v_ord),
      stars = sig_stars(p)
    )

  dat_plot <- left_join(r_long, p_long, by = c("var1", "var2"))

  ggplot(dat_plot, aes(x = var2, y = var1, fill = r)) +
    geom_tile(color = "white", linewidth = 0.6) +
    # r with 3 decimals
    geom_text(aes(label = sprintf("%.2f", r)),
              size = 3.4, color = "black") +
    # significance stars
    geom_text(aes(label = stars),
              nudge_x = 0.33, nudge_y = 0.28,
              size = 4.2, color = "black") +
    scale_fill_gradient2(
      # low = "#3B4CC0", mid = "white", high = "#B40426",
      low = '#3A6C6E', mid = "white", high = '#B37666',


      midpoint = 0, limits = c(-1, 1), name = "r"
    ) +
    coord_fixed() +
    labs(
      title = title, x = NULL, y = NULL,
      subtitle = "Stars: * p<0.05, ** p<0.01, *** p<0.001"
    ) +
    theme_minimal(base_size = 12) +
    theme(
      panel.grid = element_blank(),
      axis.text.x = element_text(angle = 35, hjust = 1),
      plot.title = element_text(face = "bold"),
      legend.position = "right"
    )
}

# ---- 6) Overall heatmap ----
overall <- corr_with_p(df, vars, method = "pearson")
p_overall <- plot_corr_heatmap(overall$r, overall$p, vars_order = vars,
                               title = "Overall correlations (expanded by topic)")
print(p_overall)

# write.csv(overall$r, "corr_overall_r.csv", row.names = TRUE)
# write.csv(overall$p, "corr_overall_p.csv", row.names = TRUE)

# # ---- 7) Per-topic heatmaps (same fixed order) ----
# topics <- sort(unique(df_long$topic))
#
# pdf("corr_heatmaps_by_topic.pdf", width = 8.2, height = 7.2)
#
# for (tp in topics) {
#   dat_tp <- df_long %>% filter(topic == tp)
#   if (nrow(dat_tp) < 10) next
#
#   res_tp <- corr_with_p(dat_tp, vars, method = "pearson")
#
#   p_tp <- plot_corr_heatmap(
#     res_tp$r, res_tp$p, vars_order = vars,
#     title = paste0("Correlations within topic: ", tp, " (n=", nrow(dat_tp), ")")
#   )
#   print(p_tp)
#
#   tp_safe <- str_replace_all(tp, "[^A-Za-z0-9_\\-]+", "_")
#   write.csv(res_tp$r, paste0("corr_", tp_safe, "_r.csv"), row.names = TRUE)
#   write.csv(res_tp$p, paste0("corr_", tp_safe, "_p.csv"), row.names = TRUE)
# }
#
# dev.off()
#
# message("Done. Fixed order applied and r formatted to 3 decimals.")







################## 词频 ##################
# setwd('D:/Fudan/Zhang Lab/language competition/diachronic_sense_modeling/data')
# load('eemd.lv/eemd_merge_1061.Rdata')
# load("revolt_inidom_uninidom_dt2.Rdata")
load('Freq_to_vec.Rdata')
result <- it1ws5_data_v2$dialogue

matplot(result$df$year, data.frame(2:(length(result$df$year)+1),1-2:(length(result$df$year)+1)),
        type = "l", col = 'white',
        # xlab = "Year", ylab = 'Proportion of Senses',
        # main = 'Proportion of Senses in "appointed"',
        xlab = "", ylab = '',
        main = "",
        ylim = c(0,10),
        lwd = 3, lty = 1)

Freq <- read.csv('D:/Fudan/Zhang Lab/language competition/lemma_freq/coha_freq.csv')
Freq[,'tokens']   # 1820-2010 每隔十年
# 3199个词都有2000


### a ###
# √ fighter
matplot(seq(1930,2010,10),(Freq_to_vec$fighter/(Freq[,'tokens'][-1])*100000)[-c(1:10)], type = "l", lwd = 3, lty = 1,
        ylim = c(0,10),
        xlab = "", ylab = '', main = "",  col = '#1F783B')
matplot(seq(1930,2010,10),Freq_to_vec$fighter[-c(1:10)], ylim = c(-.6,.6),
        xlab = "", ylab = '', main = "",lwd = 3, lty = 1, col = 'white')
lines(solve_it1ws5_v2_2w$fighter$year,solve_it1ws5_v2_2w$fighter$sigmadw,
      lwd = 3, lty = 1, col = '#E56A27')

# √ tank
matplot(seq(1900,2010,10),(Freq_to_vec$tank/Freq[-1,'tokens']*100000)[-c(1:7)], type = "l", lwd = 3, lty = 1,
        ylim = c(0,20),
        xlab = "", ylab = '', main = "", col = '#1F783B')
matplot(seq(1900,2010,10),Freq_to_vec$tank[-c(1:7)], ylim = c(-.5,.5),
        xlab = "", ylab = '', main = "",lwd = 3, lty = 1, col = 'white')
lines(solve_it1ws5_v2_2w$tank$year,-solve_it1ws5_v2_2w$tank$sigmadw,
      lwd = 3, lty = 1, col = '#E56A27')


# √ dialogue
matplot(seq(1940,2010,10),(Freq_to_vec$dialogue/Freq[,'tokens']*100000)[-c(1:12)], type = "l", lwd = 3, lty = 1,
        ylim = c(0,10),
        xlab = "", ylab = '', main = "", col = '#1F783B')
matplot(seq(1940,2010,10),Freq_to_vec$dialogue[-c(1:12)], ylim = c(-.5,.5),
        xlab = "", ylab = '', main = "",lwd = 3, lty = 1, col = 'white')
lines(solve_it1ws5_v2_2w$dialogue$year,solve_it1ws5_v2_2w$dialogue$sigmadw,
      lwd = 3, lty = 1, col = '#E56A27')


# √ peaceful
matplot(seq(1830,2010,10),(Freq_to_vec$peaceful/Freq[,'tokens']*100000)[-1], type = "l", lwd = 3, lty = 1,
        ylim = c(0,6),
        xlab = "", ylab = '', main = "",  col = '#1F783B')
matplot(seq(1830,2010,10),Freq_to_vec$peaceful[-1], ylim = c(-.8,.8),
        xlab = "", ylab = '', main = "",lwd = 3, lty = 1, col = 'white')
lines(solve_it1ws5_v2_2w$peaceful$year,solve_it1ws5_v2_2w$peaceful$sigmadw,
      lwd = 3, lty = 1, col = '#E56A27')





### b###
# √ company
matplot(seq(1820,2010,10),Freq_to_vec$company/Freq[,'tokens']*100000, type = "l", lwd = 3, lty = 1,
        ylim = c(0,100),
        xlab = "", ylab = '', main = "", col = '#1F783B')
matplot(seq(1820,2010,10),Freq_to_vec$company, ylim = c(-.15,.15),
        xlab = "", ylab = '', main = "",lwd = 3, lty = 1, col = 'white')
lines(solve_it1ws5_v2_2w$company$year,solve_it1ws5_v2_2w$company$sigmadw,
      lwd = 3, lty = 1, col = '#E56A27')

# √staff
matplot(seq(1830,2010,10),(Freq_to_vec$staff/Freq[,'tokens']*100000)[-1], type = "l", lwd = 3, lty = 1,
        ylim = c(0,40),
        xlab = "", ylab = '', main = "", col = '#1F783B')
matplot(seq(1830,2010,10),Freq_to_vec$staff[-1], ylim = c(-.4,.4),
        xlab = "", ylab = '', main = "",lwd = 3, lty = 1, col = 'white')
lines(solve_it1ws5_v2_2w$staff$year,solve_it1ws5_v2_2w$staff$sigmadw,
      lwd = 3, lty = 1, col = '#E56A27')


# √ bus
matplot(seq(1900,2010,10),(Freq_to_vec$bus/Freq[-1,'tokens']*100000)[-c(1:7)], type = "l", lwd = 3, lty = 1,
        ylim = c(0,30),
        xlab = "", ylab = '', main = "", col = '#1F783B')
matplot(seq(1900,2010,10),Freq_to_vec$bus[-c(1:7)], ylim = c(-0.2,.2),
        xlab = "", ylab = '', main = "",lwd = 3, lty = 1, col = 'white')
lines(solve_it1ws5_v2_2w$bus$year,solve_it1ws5_v2_2w$bus$sigmadw,
      lwd = 3, lty = 1, col = '#E56A27')



# √ engine
matplot(seq(1850,2010,10),(Freq_to_vec$engine/Freq[,'tokens']*100000)[-c(1:3)], type = "l", lwd = 3, lty = 1,
        ylim = c(0,20),
        xlab = "", ylab = '', main = "", col = '#1F783B')
matplot(seq(1850,2010,10),Freq_to_vec$engine[-c(1:3)], ylim = c(-.2,.2),
        xlab = "", ylab = '', main = "",lwd = 3, lty = 1, col = 'white')
lines(solve_it1ws5_v2_2w$engine$year,solve_it1ws5_v2_2w$engine$sigmadw,
      lwd = 3, lty = 1, col ='#E56A27')





### c ###


# √scarce
matplot(seq(1830,1920,10),(Freq_to_vec$scarce/Freq[,'tokens']*100000)[2:11], type = "l", lwd = 3, lty = 1,
        ylim = c(0,50),
        xlab = "", ylab = '', main = "", col = '#1F783B')
matplot(seq(1830,1920,10),Freq_to_vec$scarce[2:11], ylim = c(-.5,.5),
        xlab = "", ylab = '', main = "",lwd = 3, lty = 1, col = 'white')
lines(solve_it1ws5_v2_2w$scarce$year,solve_it1ws5_v2_2w$scarce$sigmadw,
      lwd = 3, lty = 1, col = '#E56A27')



# retire
matplot(seq(1820,2010,10),Freq_to_vec$retire/Freq[,'tokens']*100000, type = "l", lwd = 3, lty = 1,
        ylim = c(0,50),
        xlab = "", ylab = '', main = "", col = '#1F783B')
matplot(seq(1820,2010,10),Freq_to_vec$retire, ylim = c(-.3,.3),
        xlab = "", ylab = '', main = "",lwd = 3, lty = 1, col = 'white')
lines(solve_it1ws5_v2_2w$retire$year,solve_it1ws5_v2_2w$retire$sigmadw,
      lwd = 3, lty = 1, col = '#E56A27')

# √channel
matplot(seq(1820,2010,10),Freq_to_vec$channel/Freq[,'tokens']*100000, type = "l", lwd = 3, lty = 1,
        ylim = c(0,50),
        xlab = "", ylab = '', main = "", col = '#1F783B')
matplot(seq(1820,2010,10),Freq_to_vec$channel, ylim = c(-.5,.5),
        xlab = "", ylab = '', main = "",lwd = 3, lty = 1, col = 'white')
lines(solve_it1ws5_v2_2w$channel$year,solve_it1ws5_v2_2w$channel$sigmadw,
      lwd = 3, lty = 1, col = '#E56A27')


# √faculty
matplot(seq(1820,2010,10),Freq_to_vec$faculty/Freq[,'tokens']*100000, type = "l", lwd = 3, lty = 1,
        ylim = c(0,50),
        xlab = "", ylab = '', main = "", col = '#1F783B')
matplot(seq(1820,2010,10),Freq_to_vec$faculty, ylim = c(-.5,.5),
        xlab = "", ylab = '', main = "",lwd = 3, lty = 1, col = 'white')
lines(solve_it1ws5_v2_2w$faculty$year,solve_it1ws5_v2_2w$faculty$sigmadw,
      lwd = 3, lty = 1, col = '#E56A27')


######################### 词频无法反映 ####################
# industry, company, corporation
# - staff, expert, package

max(Freq_to_vec$company)

matplot(seq(1820,2010,10),Freq_to_vec$industry/Freq[,'tokens']*100000, type = "l", lwd = 3, lty = 1,
        # ylim = c(0,5500),
        xlab = "", ylab = '', main = "", col = '#1F783B')
matplot(seq(1820,2010,10),Freq_to_vec$industry, ylim = c(-1,1),
        xlab = "", ylab = '', main = "",lwd = 3, lty = 1, col = 'white')
lines(solve_it1ws5_v2_2w$faculty$year,solve_it1ws5_v2_2w$faculty$sigmadw,
      lwd = 3, lty = 1, col = '#E56A27')

# 5
matplot(seq(1820,2010,10),Freq_to_vec$company/Freq[,'tokens']*100000, type = "l", lwd = 3, lty = 1,
        ylim = c(0,12000),
        xlab = "", ylab = '', main = "", col = '#1F783B')
matplot(seq(1820,2010,10),Freq_to_vec$company, ylim = c(-1,1),
        xlab = "", ylab = '', main = "",lwd = 3, lty = 1, col = 'white')
lines(eemd_merge_1061$company$year,eemd_merge_1061$company[,'diffusion']-eemd_merge_1061$company[,'IMF 1'],
      lwd = 3, lty = 1, col = '#E56A27')

dev.new()
matplot(seq(1820,2010,10),Freq_to_vec$corporation/Freq[,'tokens']*100000, type = "l", lwd = 3, lty = 1,
        ylim = c(0,5500),
        xlab = "", ylab = '', main = "", col = '#1F783B')
matplot(seq(1820,2010,10),Freq_to_vec$corporation, ylim = c(-.3,.3),
        xlab = "", ylab = '', main = "",lwd = 3, lty = 1, col = 'white')
lines(eemd_merge_1061$corporation$year,eemd_merge_1061$corporation[,'diffusion']-eemd_merge_1061$corporation[,'IMF 1'],
      lwd = 3, lty = 1, col = '#E56A27')

# 5
matplot(seq(1820,2010,10),Freq_to_vec$staff/Freq[,'tokens']*100000, type = "l", lwd = 3, lty = 1,
        ylim = c(0,5500),
        xlab = "", ylab = '', main = "", col = '#1F783B')
matplot(seq(1820,2010,10),Freq_to_vec$staff, ylim = c(-.3,.3),
        xlab = "", ylab = '', main = "",lwd = 3, lty = 1, col = 'white')
lines(eemd_merge_1061$staff$year,eemd_merge_1061$staff[,'diffusion']-eemd_merge_1061$staff[,'IMF 1'],
      lwd = 3, lty = 1, col = '#E56A27')


matplot(seq(1820,2010,10),Freq_to_vec$expert/Freq[,'tokens']*100000, type = "l", lwd = 3, lty = 1,
        ylim = c(0,5500),
        xlab = "", ylab = '', main = "", col = '#1F783B')
matplot(seq(1820,2010,10),Freq_to_vec$expert, ylim = c(-.3,.3),
        xlab = "", ylab = '', main = "",lwd = 3, lty = 1, col = 'white')
lines(eemd_merge_1061$expert$year,eemd_merge_1061$expert[,'diffusion']-eemd_merge_1061$expert[,'IMF 1'],
      lwd = 3, lty = 1, col = '#E56A27')

################# change or stable(看sd)##############
library(dplyr)
library(stats) # 用于 sd 和 quantile

# 步骤 1: 收集所有词义的波动性 (SD)
all_sense_sds <- c()
all_sense_word_names <- c()
all_sense_indices <- c()
sense_counter <- 1

print("Step 1: Calculating SD for all senses...")

# 遍历每个词 (Word)
for (i in 1:length(it1ws5_data_v2)) {
  word_name <- names(it1ws5_data_v2)[i]
  senses_list <- it1ws5_data_v2[[i]]$senses

  if (is.null(senses_list) || length(senses_list) == 0) next

  # 遍历该词的每个词义 (Sense)
  for (j in 1:length(senses_list)) {
    sense_data <- senses_list[[j]]

    # 记录词义信息，用于后续匹配
    all_sense_word_names[sense_counter] <- word_name
    all_sense_indices[sense_counter] <- j

    # 确保有足够的数据
    if (is.null(sense_data$y) || length(sense_data$y) < 2) {
      all_sense_sds[sense_counter] <- NA
    } else {
      # 计算标准差
      all_sense_sds[sense_counter] <- sd(sense_data$y, na.rm = TRUE)
    }
    sense_counter <- sense_counter + 1
  }
}

# 步骤 2: 确定波动性阈值
# 我们使用50%分位数作为“高波动”的门槛
# 注意：如果你想得到更多"changing"词，可以调低这个值，比如 0.5 (中位数)
quantile_threshold <- 0.5
sd_threshold <- quantile(all_sense_sds, quantile_threshold, na.rm = TRUE)

print(paste("Volatility Threshold (", quantile_threshold * 100, "th percentile):", sd_threshold))

# 步骤 3: 判定每个词义(Sense)是否变化
# 将所有信息合并到一个数据框中
all_sense_stats <- data.frame(
  word_name = all_sense_word_names,
  sense_index = all_sense_indices,
  sd_value = all_sense_sds,
  stringsAsFactors = FALSE
)

# 判定每个 *词义* 是否变化
all_sense_stats <- all_sense_stats %>%
  mutate(
    # 判定标准：SD > 阈值
    is_changing_sense = ifelse(!is.na(sd_value) & sd_value > sd_threshold, TRUE, FALSE)
  )

# 步骤 4: 汇总到“词” (Word) 级别
word_summary <- all_sense_stats %>%
  filter(!is.na(word_name)) %>%
  group_by(word_name) %>%
  # 核心逻辑：
  # 只要这个词 *任何一个* 词义是 changing，这个词就是 changing
  summarise(is_changing_word = any(is_changing_sense, na.rm = TRUE))

# 步骤 5: 获取最终的词列表
wil_changeword_all <- word_summary$word_name[word_summary$is_changing_word]
wil_stableword_all <- word_summary$word_name[!word_summary$is_changing_word]

print("--- Results (SD-Only Method) ---")
print(paste("Changing words:", length(wil_changeword_all)))
print(paste("Stable words:", length(wil_stableword_all)))

# (可选) 打印出 "changing SENSES" 的比例
changing_senses_count <- sum(all_sense_stats$is_changing_sense, na.rm = TRUE)
total_senses_tested <- sum(!is.na(all_sense_stats$sd_value))
print(paste("Total senses tested:", total_senses_tested))
print(paste("Percentage of changing SENSES:", round(changing_senses_count / total_senses_tested, 4)))

length(wil_changeword_all)   # 2681
length(wil_stableword_all)   # 518

sd_changeword_all=wil_changeword_all
sd_stableword_all=wil_stableword_all
save(sd_changeword_all, file = '2025/sd_changeword_all.Rdata')
save(sd_stableword_all, file = '2025/sd_stableword_all.Rdata')

# 找为0的
## Count how many sublists in it1ws5_data_v2 have zeros in their df (sense columns)

count_df_with_zero <- function(dat, df_name = "df") {
  stopifnot(is.list(dat))

  has_zero <- vapply(dat, function(item) {
    if (!is.list(item) || is.null(item[[df_name]])) return(FALSE)
    df <- item[[df_name]]
    if (!(is.data.frame(df) || is.matrix(df))) return(FALSE)

    # keep only numeric columns (e.g., sense1, sense2, ...)
    num_mat <- as.matrix(df[, vapply(df, is.numeric, logical(1)), drop = FALSE])
    if (length(num_mat) == 0) return(FALSE)

    any(num_mat == 0, na.rm = TRUE)
  }, logical(1))

  list(
    n_with_zero = sum(has_zero),
    names_with_zero = names(dat)[has_zero],
    indicator = has_zero
  )
}

res <- count_df_with_zero(it1ws5_data_v2, df_name = "df")
res$n_with_zero          # 子列表数目
head(res$names_with_zero) # 前几个子列表名称（可选）

null_words <- c(
  "armed","beat","castle","china","consult","cottage","cruel","destruction",
  "dozen","equality","excited","expectation","handful","harmony","honest",
  "indifference","inspired","intention","justify","maybe","participate",
  "patience","senator","snap","somebody","sunshine","temporary","tribe",
  "unique","unnecessary","wise"
)
in_change  <- null_words %in% sd_changeword_all
in_stable  <- null_words %in% sd_stableword_all

## 如果 change / stable 是互斥的，可以定义一个分类变量
group <- ifelse(in_change, "change",
                ifelse(in_stable, "stable", "other"))

## 1) summary：31 个词里，各类的计数
summary_table_counts <- as.data.frame(table(group))
summary_table_counts

########### 词义层面changing/stable ############
library(dplyr)
library(stats) # sd, quantile
## 只在“非 NULL 词”上做分析
target_words <- setdiff(names(it1ws5_data_v2), null_words)
length(target_words)        # 3168

##  sum up n_senses over all these words
total_n_senses <- sum(
  sapply(it1ws5_data_v2[target_words], function(x) x$n_senses),
  na.rm = TRUE
)

total_n_senses

# 步骤 1: 收集所有“词义”的波动性 (SD)
all_sense_sds        <- c()
all_sense_word_names <- c()
all_sense_indices    <- c()
all_sense_names      <- c()  # 用于存储词义的名称
sense_counter <- 1

cat("Step 1: Calculating SD for all senses (excluding NULL words)...\n")

for (word_name in target_words) {
  senses_list <- it1ws5_data_v2[[word_name]]$senses
  if (is.null(senses_list) || length(senses_list) == 0) next

  # 遍历该词的每个词义 (sense)
  for (j in seq_along(senses_list)) {
    sense_data <- senses_list[[j]]

    # 记录词义所属的词和 index
    all_sense_word_names[sense_counter] <- word_name
    all_sense_indices[sense_counter]    <- j

    # 获取该词义的名称
    all_sense_names[sense_counter] <- names(senses_list)[j]

    # 确保有足够的数据
    if (is.null(sense_data$y) || length(sense_data$y) < 2) {
      all_sense_sds[sense_counter] <- NA_real_
    } else {
      all_sense_sds[sense_counter] <- sd(sense_data$y, na.rm = TRUE)
    }
    sense_counter <- sense_counter + 1
  }
}

# 步骤 2: 确定波动性阈值
# 这里用 50% 分位数作为 “high-volatility” 的门槛
quantile_threshold <- 0.5
sd_threshold <- quantile(all_sense_sds, quantile_threshold, na.rm = TRUE)

cat("Volatility threshold (", quantile_threshold * 100,
    "th percentile): ", sd_threshold, "\n", sep = "")


# 步骤 3: 构造“词义层面”的结果表
all_sense_stats <- data.frame(
  word_name   = all_sense_word_names,
  sense_name  = all_sense_names,  # 使用词义名称替代 sense_index
  sd_value    = all_sense_sds,
  stringsAsFactors = FALSE
)

all_sense_stats <- all_sense_stats %>%
  mutate(
    is_changing_sense = !is.na(sd_value) & sd_value > sd_threshold
  )
# 现在这里已经是“词义层面”的判断结果：
# 每一行 = 一个 word + 一个 sense_index
# is_changing_sense == TRUE 表示该词义有高波动

## 有波动的词义列表：
changing_senses <- all_sense_stats %>%
  filter(is_changing_sense)
dim(changing_senses)  # 7173
save(changing_senses, file = '2025/changing_senses.Rdata')
## 无波动的词义列表：
stable_senses <- setdiff(all_sense_stats,changing_senses)
dim(stable_senses)  # 7174
save(stable_senses, file = '2025/stable_senses.Rdata')

## “词级”的 changing / stable：
word_summary <- all_sense_stats %>%
  filter(!is.na(word_name)) %>%
  group_by(word_name) %>%
  summarise(is_changing_word = any(is_changing_sense, na.rm = TRUE),
            .groups = "drop")

wil_changeword_all  <- word_summary$word_name[word_summary$is_changing_word]
wil_stableword_all  <- word_summary$word_name[!word_summary$is_changing_word]

########### 在changing_senses中找出属于dominant sense的词义 ############

# 加载必要的库
library(dplyr)

# 读取数据（如果还没有加载）
# load('2025/changing_senses.Rdata')  # 如果还没有加载

# 步骤1: 从原始数据中直接计算每个词的dominant sense
cat("Step 1: Calculating dominant sense for each word from raw data...\n")

# 创建存储dominant sense的数据框
dominant_senses_list <- list()

# 遍历target_words（非NULL词）
for (word_name in target_words) {
  word_data <- it1ws5_data_v2[[word_name]]

  if (is.null(word_data$senses) || length(word_data$senses) == 0) {
    next  # 跳过没有词义的词
  }

  senses_list <- word_data$senses
  max_y1 <- -Inf
  dominant_sense_name <- NULL

  # 遍历该词的所有词义，找到y[1]最大的词义
  for (sense_name in names(senses_list)) {
    sense_data <- senses_list[[sense_name]]

    # 检查是否有y数据
    if (!is.null(sense_data$y) && length(sense_data$y) >= 1) {
      y_first <- sense_data$y[1]

      # 如果y_first更大，更新dominant sense
      if (!is.na(y_first) && y_first > max_y1) {
        max_y1 <- y_first
        dominant_sense_name <- sense_name
      }
    }
  }

  # 如果找到了dominant sense，添加到列表
  if (!is.null(dominant_sense_name)) {
    dominant_senses_list[[word_name]] <- data.frame(
      word_name = word_name,
      sense_name = dominant_sense_name,
      y_first = max_y1,
      stringsAsFactors = FALSE
    )
  }
}

# 合并所有dominant sense
dominant_senses_df <- do.call(rbind, dominant_senses_list)
rownames(dominant_senses_df) <- NULL

cat("Total number of words with dominant sense:", nrow(dominant_senses_df), "\n")

# 步骤2: 在changing_senses中标记哪些是dominant sense
cat("Step 2: Identifying which changing senses are dominant senses...\n")

# 方法1: 添加标记列到changing_senses
changing_senses_marked <- changing_senses %>%
  left_join(
    dominant_senses_df %>% select(word_name, sense_name) %>% mutate(is_dominant = TRUE),
    by = c("word_name", "sense_name")
  ) %>%
  mutate(
    is_dominant = ifelse(is.na(is_dominant), FALSE, is_dominant)
  )

# 统计结果
dominant_in_changing <- changing_senses_marked %>%
  filter(is_dominant)

cat("\n=== 统计结果 ===\n")
cat("Total changing senses:", nrow(changing_senses), "\n")
cat("Changing senses that are dominant:", nrow(dominant_in_changing), "\n")
cat("Percentage of dominant senses in changing senses:",
    round(nrow(dominant_in_changing) / nrow(changing_senses) * 100, 2), "%\n")

# 方法2: 直接找到changing senses中的dominant senses
changing_and_dominant <- inner_join(
  changing_senses,
  dominant_senses_df,
  by = c("word_name", "sense_name")
)

cat("\n=== Changing senses that are also dominant senses ===\n")
print(head(changing_and_dominant, 10))
cat("\nTotal:", nrow(changing_and_dominant), "\n")

# 步骤3: 分析dominant sense的change情况
cat("\nStep 3: Analyzing change status of dominant senses...\n")

# 为dominant senses添加change状态
dominant_senses_with_status <- dominant_senses_df %>%
  left_join(
    changing_senses %>% select(word_name, sense_name) %>% mutate(is_changing = TRUE),
    by = c("word_name", "sense_name")
  ) %>%
  mutate(
    is_changing = ifelse(is.na(is_changing), FALSE, is_changing)
  )

# 统计dominant senses的change情况
dominant_change_stats <- dominant_senses_with_status %>%
  group_by(is_changing) %>%
  summarise(
    count = n(),
    percentage = round(n() / nrow(dominant_senses_with_status) * 100, 2)
  )

cat("\n=== Dominant senses change statistics ===\n")
print(dominant_change_stats)

# 步骤4: 创建详细的分析报告
cat("\nStep 4: Creating detailed analysis report...\n")

# 计算一些关键指标
total_words_with_dominant <- nrow(dominant_senses_df)
total_changing_senses <- nrow(changing_senses)
changing_dominant_senses <- nrow(changing_and_dominant)

# 创建报告数据框
analysis_report <- data.frame(
  Metric = c(
    "Total words with dominant sense",
    "Total changing senses",
    "Changing senses that are dominant",
    "Percentage of changing senses that are dominant",
    "Dominant senses that are changing",
    "Percentage of dominant senses that are changing"
  ),
  Value = c(
    total_words_with_dominant,
    total_changing_senses,
    changing_dominant_senses,
    paste0(round(changing_dominant_senses / total_changing_senses * 100, 2), "%"),
    sum(dominant_senses_with_status$is_changing),
    paste0(round(mean(dominant_senses_with_status$is_changing) * 100, 2), "%")
  ),
  stringsAsFactors = FALSE
)

print(analysis_report)

# 步骤5: 保存结果
cat("\nStep 5: Saving results...\n")

# 保存标记后的changing_senses
save(changing_senses_marked, file = '2025/changing_senses_marked_with_dominant.Rdata')

# 保存changing_and_dominant
save(changing_and_dominant, file = '2025/changing_and_dominant_senses.Rdata')

# 保存dominant senses with status
save(dominant_senses_with_status, file = '2025/dominant_senses_with_change_status.Rdata')

# 保存为CSV以便查看
write.csv(changing_senses_marked,
          file = '2025/changing_senses_marked_with_dominant.csv',
          row.names = FALSE)

write.csv(changing_and_dominant,
          file = '2025/changing_and_dominant_senses.csv',
          row.names = FALSE)

write.csv(dominant_senses_with_status,
          file = '2025/dominant_senses_with_change_status.csv',
          row.names = FALSE)

# 步骤6: 查看一些示例
cat("\n=== Examples ===")
cat("\n\nFirst 10 changing senses that are dominant:\n")
print(head(changing_and_dominant, 10))

cat("\n\nFirst 10 changing senses that are NOT dominant:\n")
changing_not_dominant <- changing_senses_marked %>%
  filter(!is_dominant)
print(head(changing_not_dominant, 10))

cat("\n\nFirst 10 dominant senses that are NOT changing:\n")
dominant_not_changing <- dominant_senses_with_status %>%
  filter(!is_changing)
print(head(dominant_not_changing, 10))

# 步骤7: 词义层面的汇总统计
cat("\n\nStep 7: Sense-level summary statistics...\n")

sense_summary <- changing_senses_marked %>%
  group_by(is_dominant) %>%
  summarise(
    count = n(),
    mean_sd = mean(sd_value, na.rm = TRUE),
    median_sd = median(sd_value, na.rm = TRUE),
    min_sd = min(sd_value, na.rm = TRUE),
    max_sd = max(sd_value, na.rm = TRUE)
  )

print(sense_summary)

# 保存汇总统计
save(sense_summary, file = '2025/sense_summary_by_dominance.Rdata')
write.csv(sense_summary, file = '2025/sense_summary_by_dominance.csv', row.names = FALSE)

cat("\n=== Analysis complete! ===\n")
cat("Results saved in the '2025' directory:\n")
cat("1. changing_senses_marked_with_dominant.Rdata/csv - changing senses with dominant flag\n")
cat("2. changing_and_dominant_senses.Rdata/csv - changing senses that are dominant\n")
cat("3. dominant_senses_with_change_status.Rdata/csv - dominant senses with change status\n")
cat("4. sense_summary_by_dominance.Rdata/csv - summary statistics by dominance\n")
########### 输出txt格式 ###########
names(sde_it1ws5_v2_2w_parmean)[sapply(sde_it1ws5_v2_2w_parmean, is.null)]
names(solve_it1ws5_v2_2w)[sapply(solve_it1ws5_v2_2w, is.null)]
# 31NULL  01
# 批量修改每个子数据框的列名
solve_it1ws5_v2 <- solve_it1ws5_v2_2w[!sapply(solve_it1ws5_v2_2w, is.null)]
solve_it1ws5_v2 <- lapply(solve_it1ws5_v2, function(df) {
  colnames(df)[colnames(df) == "ode"] <- "drift"
  colnames(df)[colnames(df) == "sigmadw"] <- "diffusion"
  return(df)
})   # 3168 words


options(max.print = .Machine$integer.max)
sink('EC_df.txt')
EC_df
sink()


write.table(
  all_sense_stats,
  file      = "all_sense_stats.txt",
  sep       = "\t",      # 字段之间用 tab 分隔
  quote     = FALSE,     # 字符串不加引号
  row.names = FALSE      # 不输出行名
)

## 导出 word_summary 为 txt（制表符分隔）
write.table(
  word_summary,
  file      = "word_summary.txt",
  sep       = "\t",
  quote     = FALSE,
  row.names = FALSE
)

## 导出 word_summary 为 txt（制表符分隔）
write.table(
  EC_df,
  file      = "EC_df.txt",
  sep       = "\t",
  quote     = FALSE,
  row.names = FALSE
)

mean_para <- sde_it1ws5_v2_2w_parmean[!names(sde_it1ws5_v2_2w_parmean) %in% null_words]
library(purrr)
df_mean_para <- map_dfr(mean_para, ~ as.data.frame(t(.x)), .id = "word")
colnames(df_mean_para) <- c("word", "r1", "sigma1", "c")
write.csv(df_mean_para, "mean_para.csv", row.names = FALSE)
############## 词义分布直方图 ###########
## 2. 提取非 NULL 词的 n_senses
target_words <- setdiff(names(it1ws5_data_v2), null_words)

n_senses_vec <- sapply(it1ws5_data_v2[target_words], function(x) x$n_senses)

## 3. 统计每个 n_senses 对应多少词
sense_count_tab <- table(n_senses_vec)

## 4. 计算百分比
sense_count_df <- data.frame(
  n_senses = as.numeric(names(sense_count_tab)),
  count    = as.numeric(sense_count_tab)
)
sense_count_df$percent <- sense_count_df$count / sum(sense_count_df$count) * 100

## 5. 画图（仿你的示例）
op <- par(mar = c(4, 4, 3, 1))  # 适当缩小边距

bar_mid <- barplot(
  height    = sense_count_df$percent,
  names.arg = sense_count_df$n_senses,
  col       = "lightblue",
  border    = NA,
  ylim      = c(0, max(sense_count_df$percent) * 1.2),
  xlab      = "Sense Number of Polysemous Words",
  ylab      = "Percentage (%)",
  main      = "Distribution of Words by Sense Number",
  axes      = TRUE  # 添加坐标轴
)

## 在柱子上方标注具体词数
# 添加横坐标数字标注在每个柱子下方
text(
  x      = bar_mid,
  y      = rep(0, length(bar_mid)), # 设置位置为0，显示在底部
  labels = sense_count_df$n_senses,  # 显示每个词义数
  pos    = 1,       # 下方
  cex    = 0.7
)

# 在柱子上方标注具体词数
text(
  x      = bar_mid,
  y      = sense_count_df$percent,
  labels = sense_count_df$count,
  pos    = 3,       # 上方
  cex    = 0.7
)
# 绘制框线
box()

par(op)



############### 复制子 #####################
rep.log.cond.pdf <- function(xt1, xt2, x1, x2, dt, drift.vec, diff.vec){

  # --- 开始修改 ---

  # 1. 提取参数和当前状态
  # drift.vec[1] 是 r (增长率)
  # drift.vec[2] 是 sigma (竞争系数)
  r <- drift.vec[1]
  s <- drift.vec[2]
  # xt1 是当前的状态向量 c(x1, x2)
  x1_current <- xt1[1]
  x2_current <- xt1[2]

  # 2. 定义适应度 (Fitness) F1 和 F2
  #    (基于简化的 Lotka-Volterra 模型)
  f1 <- r * (1 - x1_current - s * x2_current)
  f2 <- r * (1 - x2_current - s * x1_current)

  # 3. 计算平均适应度 (Average Fitness) f_bar
  f_bar <- x1_current * f1 + x2_current * f2

  # 4. 计算复制子方程 (Replicator Equation) 的漂移项 (drift)
  #    x_dot = x * (f_i - f_bar)
  x1_dot <- x1_current * (f1 - f_bar)
  x2_dot <- x2_current * (f2 - f_bar)

  # 漂移向量
  mu_drift <- c(x1_dot, x2_dot)

  # --- 结束修改 ---

  # 扩散项 (Diffusion term)
  diff <- c(diff.vec, -diff.vec)
  Sigma <- diag(diff)

  # 时间步长 (dt)
  # 注意：您在函数中将 dt 硬编码为 2。
  # 您的 MCMC 循环中没有传递 dt，而是从 result$df 中计算了 time.diff
  # 但 log.cond.pdf 函数又没有使用传入的 dt 参数 (除了硬编码的 dt=2)。
  # 这里我暂时保留您硬编码的 dt = 2
  dt_step <- 2
  # 如果您想使用实际的时间间隔，您应该将 dt (即 time.diff[i]) 传递给 log.cond.pdf
  # 并在 loss 函数中修改:
  # loss <- log.cond.pdf(xt1, xt2, x1, x2, time.diff[i], drift.vec, diff.vec)
  # 然后在这里使用: dt_step <- dt

  # 欧拉法计算期望均值 (mu) 和方差 (Var)
  mu <- xt1 + mu_drift * dt_step
  Var <- dt_step * Sigma %*% t(Sigma)

  # 计算对数似然
  k <- length(xt1)
  if (sum(log(svd(Var)$d)) == -Inf) {log.f.xt2.xt1 <- -1e+10}
  else {log.f.xt2.xt1 <- (-k/2 * log(2*pi) - 0.5*sum(log(svd(Var)$d)) -
                            0.5*(t(xt2 - mu) %*% ginv(Var) %*% (xt2 - mu)))
  }

  return(log.f.xt2.xt1)
}



############### 与ODE比较 ##############
ode_lp <- lapply(ode_it1ws5_v2_2w, function(ls){ls$ode_lpmanumean}) %>% unlist()
sde_lp <- lapply(sde_it1ws5_v2_2w, function(ls){ls$sde_lpmanumean}) %>% unlist()
rep_lp <- lapply(rep_it1ws5_v2_2w, function(ls){ls$rep_lpmanumean}) %>% unlist()


summary(rep_lp)
save(rep_lp, file='rep_lp.Rdata')
logbf <- Map("-", sde_lp, ode_lp)
logbf <- Map("-",rep_lp, sde_lp)
logbf <- Map("-",sde_lp, rep_lp)




logBF <- unlist(logbf)
median(logBF)
quantile(logBF, 0.25)  # 计算第一四分位数
quantile(logBF, 0.75)

logBF_2 <- logBF[names(it1ws5_data_v2_n2)] # n=2
logBF_2 <- logBF[names(it1ws5_data_v2_n2_sense2_gt)]# n=2且交叉

wilcox.test(logBF_2, mu = 0, alternative = "greater", conf.int = TRUE)



# 将logBF_2转换为向量
logbf_values <- unlist(logBF_2)

# 使用IQR方法识别和去除离群值
remove_outliers_iqr <- function(x) {
  Q1 <- quantile(x, 0.25, na.rm = TRUE)
  Q3 <- quantile(x, 0.75, na.rm = TRUE)
  IQR <- Q3 - Q1
  lower_bound <- Q1 - 1.5 * IQR
  upper_bound <- Q3 + 1.5 * IQR
  x[x >= lower_bound & x <= upper_bound]
}

# 去除离群值
logbf_clean <- remove_outliers_iqr(logbf_values)

# 创建数据框用于绘图
logbf_clean_df <- data.frame(logBF = logbf_clean)

# 绘制箱线图（无离群值）
p1 <- ggplot(logbf_clean_df, aes(y = logBF)) +
  geom_boxplot(fill = "lightblue", alpha = 0.7, outlier.shape = NA) +
  labs(title = "Log Bayes Factor分布 (离群值已去除)",
       y = "log(Bayes Factor)") +
  theme_minimal()

print(p1)
dev.new()
boxplot(
  # unlist(logBF_2),
  logbf_clean_df,
        col = "skyblue", border = "black", background = "white",
        axes = FALSE, frame.plot = TRUE, horizontal = FALSE,
        xlab = "", ylab = "", main = "ln BF", boxwex = 0.5 )
axis(2, col = "black", lwd = 1, lty = 1)

library(ggplot2)
library(ggdist)
library(tidyquant)
library(reshape2)
library(gghalves)
logpost_df <- data.frame('word'= names(sde_it1ws5_v2_2w)[names(it1ws5_data_v2_n2_sense2_gt)],
                         'sde'= sde_lp[names(it1ws5_data_v2_n2_sense2_gt)],
                         'ode'= ode_lp[names(it1ws5_data_v2_n2_sense2_gt)])    # 1043行 3101行
logpost_df <- data.frame('word'= names(sde_it1ws5_v2_2w)[names(it1ws5_data_v2_n2_sense2_gt)],
                         'sde'= sde_lp[names(it1ws5_data_v2_n2_sense2_gt)],
                         'rep'= rep_lp[names(it1ws5_data_v2_n2_sense2_gt)])    # 1043行 3101行
logpost_df_melt <- melt(logpost_df,id='word')

wilcox.test(sde_lp,ode_lp)

dev.new()


ordercolors<-c("coral1","lightslateblue","olivedrab3","goldenrod1","lightgray")

ggplot(data = logpost_df_melt,
       aes(x=variable, y=value, fill=variable)) +
  geom_half_violin(color = "black", side = "right",
                   position = position_nudge(x = .1, y = 0), width = 1.0, cex = 0.6) +
  # 加入箱型图
  geom_half_boxplot(color = "black", width = 0.09,
                    position = position_nudge(x = .1, y = 0)) +
  # 自定义颜色
  scale_fill_manual(values = c("#9cc37b","#d06128")) +
  scale_color_manual(values = c("#9cc37b","#d06128")) +
  # scale_x_discrete(name = "") +
  # scale_y_continuous(limits = c(4,8),
  #                    breaks = seq(4,8,1))+
  # 主题设置
  theme(panel.background = element_blank(),
        panel.border = element_rect(fill = "NA", color = "black", size = 0.6),
        legend.position = "none",
        axis.ticks = element_line(colour = "black", size = 0.6),
        axis.text = element_text(colour = "black", size = 14),
        axis.title.y = element_text(size = 14, colour = "black")) +
  labs(y = "")


################ 算mse & rho ##############
msecorr_sde <- data.frame(mse = numeric(), r = numeric(), p = numeric())
for (i in 1:length(it1ws5_data_v2)) {
  tryCatch({
    print(i)
    result <- it1ws5_data_v2[[i]]
    sde.res <- solve_it1ws5_v2_2w[[i]]
    corr <- cor.test(unlist(result$df$sense1),unlist(sde.res$sde),method = 'spearman')  # corr
    mse.sde <- mean((unlist(result$df$sense1)-unlist(sde.res$sde))^2)
    msecorr_sde <- rbind(msecorr_sde, data.frame(mse = mse.sde,
                                                 r = corr$estimate,
                                                 p = corr$p.value))
  },  error = function(e) {
    return(NULL)
  })
}
rownames(msecorr_sde) <- names(solve_it1ws5_v2_2w)[!sapply(solve_it1ws5_v2_2w, is.null)]
save(msecorr_sde, file = '2025/msecorr_sde.Rdata')


load('2025/ode_it1ws5_v2_2w.Rdata')
ode_par <- lapply(ode_it1ws5_v2_2w,function(ls){ls$ode_parmean})
save(ode_par, file = '2025/ode_par.Rdata')

library(deSolve)          # solve ode
simulate_ode <- function(r1, sigma1) {
  ode_model <- function(t, state, parameters) {
    with(as.list(c(state, parameters)), {
      dX_dt <- r1 * state * (1 - state - sigma1 * (1 - state))
      return(list(dX_dt))
    })
  }
  data_obs <- data.frame(N = nrow(result$df), X_obs = result$df$sense1, year = result$df$year)
  initial_state <- data_obs$X_obs[1]
  ode_result <- ode(y = initial_state, times = data_obs$year, func = ode_model, parms = c(r1 = r1, sigma1 = sigma1), method = "euler")
  return(ode_result)
}
msecorr_ode <- data.frame(mse = numeric(), r = numeric(), p = numeric())
for (i in 1:length(it1ws5_data_v2)) {
  tryCatch({
    result <- it1ws5_data_v2[[i]]
    (par_lv <- ode_par[[i]])

    set.seed(123)
    ode.res <- simulate_ode(par_lv[1], par_lv[2])[,2]
    mse.ode <- mean((result$df$sense1 - ode.res)^2)

    corr <- cor.test(result$df$sense1,ode.res,method = 'spearman')  # corr

    msecorr_ode <- rbind(msecorr_ode, data.frame(mse = mse.ode,
                                                 r = corr$estimate,
                                                 p = corr$p.value))
  },  error = function(e) {
    return(NULL)
  })
}
dim(msecorr_ode)

rownames(msecorr_ode) <- names(solve_it1ws5_v2_2w)
save(msecorr_ode, file = '2025/msecorr_ode.Rdata')

msecorr_sde <- filter(msecorr_sde, p<0.05)
msecorr_ode <- filter(msecorr_ode, p<0.05)
summary(msecorr_ode)

wilcox.test(msecorr_sde$mse,msecorr_ode$mse)
wilcox.test(msecorr_sde$r,msecorr_ode$r)

median(msecorr_sde$r)

msecorr_sde_df <- data.frame(msecorr_sde, label = 'sde')
msecorr_ode_df <- data.frame(msecorr_ode, label = 'ode')
df_rbind <- rbind(msecorr_sde_df,msecorr_ode_df)

df_melt <- melt(df_rbind)




mse_df <- df_melt %>% filter(variable=="mse")
mse_sde <- mse_df %>% filter(label=="sde")
mse_ode <- mse_df %>% filter(label=="ode")


ggplot(mse_df, aes(x = variable, y = value, fill = label))+
  geom_half_violin(data=mse_sde,aes(x = variable, y = value),
                   position = position_dodge(width = 1),
                   scale = 'width',
                   colour=NA,fill="#1ba7b3",
                   side = "l")+
  geom_half_violin(data=mse_ode,aes(x = variable, y = value),
                   position = position_dodge(width = 1),
                   scale = 'width',
                   colour=NA,fill="#dfb424",
                   side = "r")+
  theme(axis.text.x = element_text(angle = 45, hjust = 1),panel.background = element_rect(fill = "transparent")) +
  theme_bw()+xlab("")+ylab("Value")+
  # facet_zoom(ylim = c(0,.01))  +
  coord_trans(y = squash_axis(0.03, 0.2, 50))+
  stat_summary(data = mse_df, aes(x = variable,y = value, group = label),
               fun = median,
               fun.min = function(x){quantile(x)[2]},
               fun.max = function(x){quantile(x)[4]},
               geom = "pointrange",
               # geom = 'errorbar',
               size=0.5,
               position = position_dodge(width = 0.2))+
  theme(panel.background = element_rect(fill = "transparent"))

median(mse_sde$value)
median(mse_ode$value)

df_rbind_sig <- df_rbind %>% filter(p <= 0.05)
r_df <- melt(df_rbind_sig) %>% filter(variable=="r")
r_sde <- r_df %>% filter(label=="sde")
r_ode <- r_df %>% filter(label=="ode")
r_sde[,'value'] <- abs(r_sde[,'value'])
r_ode[,'value'] <- abs(r_ode[,'value'])

median(r_sde$value)
median(r_ode$value)

dev.new()
ggplot(r_df, aes(x = variable, y = value, fill = label))+
  geom_half_violin(data=r_sde,aes(x = variable, y = value),
                   position = position_dodge(width = 1),
                   scale = 'width',
                   colour=NA,fill="#1ba7b3",
                   side = "l")+
  geom_half_violin(data=r_ode,aes(x = variable, y = value),
                   position = position_dodge(width = 1),
                   scale = 'width',
                   colour=NA,fill="#dfb424",
                   side = "r")+
  theme(axis.text.x = element_text(angle = 45, hjust = 1),panel.background = element_rect(fill = "transparent")) +
  theme_bw()+xlab("")+ylab("r")+
  # facet_zoom(ylim = c(0,4))  +
  stat_summary(data = r_df, aes(x = variable,y = value, group = label),
               fun = median,
               fun.min = function(x){quantile(x)[2]},
               fun.max = function(x){quantile(x)[4]},
               geom = "pointrange",
               #geom = 'errorbar',
               size=0.5,
               position = position_dodge(width = 0.2))+
  theme(panel.background = element_rect(fill = "transparent"))




################ EC ##############
EC <- list()
for (i in 1:length(solve_it1ws5_v2_2w)) {
  sdsigma <- sd(solve_it1ws5_v2_2w[[i]]$sigmadw)
  sdtotal <- sd(it1ws5_data_v2[[i]]$df$sense1)

  # sdtotal <- sd(solve_it1ws5_v2_2w[[i]]$sde)
  EC[[i]] <- sdsigma/sdtotal
}
names(EC) <- names(solve_it1ws5_v2_2w)
names(EC)[sapply(EC, is.na)]

EC_df <- data.frame(
  word = names(EC)[!sapply(EC, is.na)],
  value = unlist(EC[!sapply(EC, is.na)]),
  stringsAsFactors = FALSE
)

# 加上label
library(dplyr)
library(stringr)
label_map <- label_words %>%
  mutate(words_clean = str_remove_all(words, "\\.\\.\\.")) %>%
  separate_rows(words_clean, sep = ",") %>%
  mutate(words_clean = str_trim(words_clean)) %>%
  select(label, word = words_clean)
EC_df_labeled <- EC_df %>%
  left_join(label_map, by = "word") %>%
  filter(!is.na(label), label != "？")



print(p_violin)




# 所有名字
names_sde   <- names(solve_it1ws5_v2_2w)
names_data  <- names(it1ws5_data_v2)

# 名字集合是否完全相同？
setequal(names_sde, names_data)

# 如果想看多出来或缺少哪些：
only_in_sde  <- setdiff(names_sde, names_data)
only_in_data <- setdiff(names_data, names_sde)

only_in_sde
only_in_data

# 只用交集的名字
common_names <- intersect(names_sde, names_data)

solve_it1ws5_v2 = solve_it1ws5_v2_2w
# 1）从 solve_it1ws5_v2 提取 sd(sigmadw)
sd_sigmadw <- sapply(common_names, function(nm) {
  x <- solve_it1ws5_v2[[nm]]
  if (is.null(x)) return(NA_real_)
  if (!"sigmadw" %in% colnames(x)) return(NA_real_)
  sd(x[["sigmadw"]], na.rm = TRUE)
})

# 2）从 it1ws5_data_v2 提取 sd(sense1)（每个子列表里有 df）
sd_sense1 <- sapply(common_names, function(nm) {
  x <- it1ws5_data_v2[[nm]]
  if (is.null(x)) return(NA_real_)

  # 子列表可能本身就是数据框，也可能是 list(df = ...)
  df <- if (is.data.frame(x)) x else x[["df"]]
  if (is.null(df) || !"sense1" %in% colnames(df)) return(NA_real_)

  sd(df[["sense1"]], na.rm = TRUE)
})

# 3）计算比例 sd(sigmadw) / sd(sense1)
ratio_sd <- sd_sigmadw / sd_sense1

# 整理成 data.frame
EC_df <- data.frame(
  word       = common_names,
  sd_sigmadw = sd_sigmadw,
  sd_sense1  = sd_sense1,
  EC      = ratio_sd,
  row.names  = NULL
)

EC_df$EC=as.numeric(EC_df$EC)
class(EC_df$EC)
# Assuming EC_df is your data frame
sum(EC_df[[4]] >= 0 & EC_df[[4]] <= 1, na.rm = TRUE)
################ label ##############
label_words <- read.csv('label/label_words.csv', header = TRUE, fileEncoding = "GBK")


################ 过滤有词义替换的 ##############
# 筛选 n_senses = 2 的子列表
it1ws5_data_v2_n2 <- it1ws5_data_v2[sapply(it1ws5_data_v2, function(x) x$n_senses == 2)]
length(it1ws5_data_v2_n2)  # 查看筛选后的数量
names(it1ws5_data_v2_n2)   # 查看筛选后的单词名称

# 筛选 sense2 > sense1 的子列表
it1ws5_data_v2_n2_sense2_gt <- it1ws5_data_v2_n2[
  sapply(it1ws5_data_v2_n2, function(x) {
    df <- x$df
    # 检查是否有至少一行中sense2 > sense1
    # 假设sense1在第2列，sense2在第3列
    any(df[, 3] > df[, 2], na.rm = TRUE)
  })
]

length(it1ws5_data_v2_n2_sense2_gt)
names(it1ws5_data_v2_n2_sense2_gt)



# 找出交集
common_names <- intersect(names(solve_it1ws5_v2_2w), names(it1ws5_data_v2_n2_sense2_gt))
solve_it1ws5_v2_2w_sublists <- solve_it1ws5_v2_2w[common_names]


EC_2 <- list()
for (i in 1:length(it1ws5_data_v2_n2_sense2_gt)) {
  sdsigma <- sd(solve_it1ws5_v2_2w_sublists[[i]]$sigmadw)
  sdtotal <- sd(it1ws5_data_v2_n2_sense2_gt[[i]]$df$sense1)
  EC_2[[i]] <- sdsigma/sdtotal
}
names(EC_2) <- names(it1ws5_data_v2_n2_sense2_gt)

EC_2_df <- data.frame(
  word = names(EC_2)[!sapply(EC_2, is.na)],
  value = unlist(EC_2[!sapply(EC_2, is.na)]),
  stringsAsFactors = FALSE
)
EC_2_df_labeled <- EC_2_df %>%
  left_join(label_map, by = "word") %>%
  filter(!is.na(label), label != "？")


dev.new()
ggplot(EC_2_df_labeled, aes(x = reorder(label, value, median), y = value, fill = label)) +
  geom_violin(alpha = 0.7) +
  geom_boxplot(width = 0.1, fill = "white", alpha = 0.5) +
  scale_fill_viridis_d() +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    legend.position = "none"
  ) +
  labs(
    title = "EC Values Distribution by Label Category",
    x = "Label Category",
    y = "EC Value"
  )

remove_outliers_iqr <- function(data) {
  data %>%
    group_by(label) %>%
    mutate(
      Q1 = quantile(value, 0.25, na.rm = TRUE),
      Q3 = quantile(value, 0.75, na.rm = TRUE),
      IQR = Q3 - Q1,
      lower_bound = Q1 - 1.5 * IQR,
      upper_bound = Q3 + 1.5 * IQR
    ) %>%
    filter(value >= lower_bound & value <= upper_bound) %>%
    select(-Q1, -Q3, -IQR, -lower_bound, -upper_bound)
}

# 去除离群值
EC_df_no_outliers <- EC_2_df_labeled %>% remove_outliers_iqr()

# 画图

ggplot(EC_df_no_outliers, aes(x = reorder(label, value, median), y = value, fill = label)) +
  geom_boxplot(outlier.shape = NA) +
  geom_jitter(width = 0.2, alpha = 0.5, size = 1) +
  scale_fill_viridis_d() +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    legend.position = "none"
  ) +
  labs(
    title = "EC Values (IQR Outliers Removed)",
    x = "Semantic Category",
    y = "EC Value"
  )

